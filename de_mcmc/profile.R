version="h_m1"
#our first hierarhical model for the reversal learning dataset.
verbose=TRUE
source('de_mcmc/main_m1_setup.R')

##############################################  generate data
source("de_mcmc/raw_data_reward_only.R")

##############################################  initialize
#data[[1]]
#NAMING CONVENTION
#start with the name of the parameter itself (e.g., "alpha", "beta", and so on)
#For bottom levels, ELIMINATE the specific level it refers to. So somewhat paradoxically, for the alpha value for each subject, do NOT append "s", but append any other values (e.g., "run_mu")
#For next level up, specify which distribution it takes an average of (e.g., "alpha_s") then the hyper parameter it is (e.g., mu, sigma)w
#if we treat the levels as LISTS rather than ARRAYS then we can associate arbitrary dimensions to each parameter

#NAMING CONVENTION 2
#start with the name of the parameter itself (e.g., "alpha", "beta", and so on)
#always refer to the specific level, e.g., "run" if this is for the run
#if it's an average of something within a group, then specify both
#e.g., the average across runs for a subject is alpha_r_mu_s
#if it's the conjugate, can eliminate the lower-level values, so that the group-level parameter is "alpha_s_mu"
#otherwise keep, so that we have alpha_r_sigma_s generated by alpha_r_sigma_s_{sigma hypers}
#we might end up with nothing called simply "alpha" but that's OK.
#call the bottom level transformed variable "f_alpha_s" for 'function of alpha_s'
level1.par.names=c("alpha_s",#"beta",
                   "thresh_s","tau_s")

level2.par.names<-c(paste0(level1.par.names, "_mu"),paste0(level1.par.names, "_sigma"))

par.names<-c(level1.par.names,level2.par.names)
n.pars=length(par.names)

n.chains=24
nmc=5000
burnin=1000
thin=1
keep.samples=seq(burnin,nmc,thin)
print(length(keep.samples)*n.chains)


use.optim=TRUE
optim.gamma=TRUE
migrate.prob=.1
migrate.duration=round(burnin*.25)+1
b=.001

cores=8
data<-data[seq(1,161,80)]
S=length(data)


#defining initial values
#because this time, these variables will be normal distributions, means can be zero, SDs can be ones.

#MAY NEED TO ADJUST THESE TAU VALUES IN LIGHT OF PUTTING THIS IN NORMAL DISTRIBUTION SPACE
x.init<- list()
x.init[[level1.par.names[[1]]]]<-rep(0,S)#alpha
x.init[[level1.par.names[[2]]]]<-rep(0.5,S)#thresh
x.init[[level1.par.names[3]]]=.6*(sapply(data,function(x)min(x$rt,na.rm=TRUE))) #is this the best we can do?
#mu values
for (pn in level2.par.names[1:2]){
  x.init[[pn]]<-0
}
x.init[[level2.par.names[3]]]=.6*mean(sapply(data,function(x)min(x$rt,na.rm=TRUE))) #is this the best we can do?
#sigma values
for (pn in level2.par.names[4:5]){
  x.init[[pn]]<-1
}
x.init[[level2.par.names[6]]]=.6*sd(sapply(data,function(x)min(x$rt,na.rm=TRUE))) #is this the best we can do?

n.parinstances<-length(unlist(x.init))#the list of parameter instances, counting each element of the vector parameters
##############################################  set prior

prior=NULL
# upper and lower boundaries for the concentration parameters
prior$lower=0  
prior$upper=1

########################################## run it
source(paste0("de_mcmc/model_configs/de_",version,"_config.R"))
source(paste0("de_mcmc/de_",version,"_run.R"))

#######################################################################################################################################
#run_env<-de_mcmc_execute(log.dens.like.h.m1,log.dens.prior.h.m1)#log.dens.like.f<-log.dens.like.h.m1;log.dens.prior.f<-log.dens.prior.h.m1
#log.dens.like.f<-log.dens.like.h.m1;log.dens.prior.f<-log.dens.prior.h.m1
mainDir<-getwd()
subDir=""

sfInit(parallel=TRUE, cpus=cores, type="SOCK")
#printv("setting up cluster...")
sfClusterSetupRNG()

ptm=proc.time()[3]
#printv ("running the model...")
de_m1_run(log.dens.like.f=log.dens.like.f,
          log.dens.prior.f=log.dens.prior.f)
proc.time()[3]-ptm


.dens.prior.f<-log.dens.prior.h.m1
#######################################################################################################################################
log.dens.post=function(x,use.data,prior)log.dens.prior.f(x,prior) + log.dens.like.f(x,use.data)

########################################### initialize the chains
printv("intializing chains...")
theta<<-array(NA,c(n.chains,n.pars,S))
weight<<-array(-Inf,c(n.chains,S))

colnames(theta) <- par.names
printv("exporting...")
sfExportAll(except=list("theta","weight"))

print("Optimizing...(this may take some time)")
init.pars=matrix(1,n.pars)
#I don't look through becasue we no longer have a single matrix of everything
#we have a list, some of which are across-subject values, some of which are not.
x=unlist(x.init)
temp.weight=log.dens.like.f(x,use.data=data)
new.x=x
while(temp.weight==-Inf){
  stop("I think this hasn't been set up properly. Not only are the values probably wrong but the function takes far too long to run!")
  #is this appropriate for all the variables? I'm not sure.
  new.x=rtnorm(n.parinstances,.1,0,inf.vals) #BJS to BT: why does this use a truncated normal distribution?
  warning("We use this truncated normal distribution to estimate new x values, but I'm not sure that this is appropriate for all variables.")
  
  #shouldn't we be getting the priors here?
  temp.weight=log.dens.like.f(new.x,use.data=data)
}
if(use.optim==TRUE)init.pars=optim(new.x,function(x,...)-log.dens.like.f(x,...),use.data=data,control=list("maxit"=1000))$par
if(use.optim==FALSE)init.pars=new.x
print(paste("Optimization Complete."))
#}
