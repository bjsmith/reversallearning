version="h_m1"
#our first hierarhical model for the reversal learning dataset.
verbose=TRUE
source('de_mcmc/main_m1_setup.R')

##############################################  generate data
source("de_mcmc/raw_data_reward_only.R")

##############################################  initialize
#data[[1]]
#NAMING CONVENTION
#start with the name of the parameter itself (e.g., "alpha", "beta", and so on)
#For bottom levels, ELIMINATE the specific level it refers to. So somewhat paradoxically, for the alpha value for each subject, do NOT append "s", but append any other values (e.g., "run_mu")
#For next level up, specify which distribution it takes an average of (e.g., "alpha_s") then the hyper parameter it is (e.g., mu, sigma)w
#if we treat the levels as LISTS rather than ARRAYS then we can associate arbitrary dimensions to each parameter

#NAMING CONVENTION 2
#start with the name of the parameter itself (e.g., "alpha", "beta", and so on)
#always refer to the specific level, e.g., "run" if this is for the run
#if it's an average of something within a group, then specify both
#e.g., the average across runs for a subject is alpha_r_mu_s
#if it's the conjugate, can eliminate the lower-level values, so that the group-level parameter is "alpha_s_mu"
#otherwise keep, so that we have alpha_r_sigma_s generated by alpha_r_sigma_s_{sigma hypers}
#we might end up with nothing called simply "alpha" but that's OK.
#call the bottom level transformed variable "f_alpha_s" for 'function of alpha_s'
level1.par.names=c("alpha_s",#"beta",
                   "thresh_s","tau_s")

level2.par.names<-c(paste0(level1.par.names, "_mu"),paste0(level1.par.names, "_sigma"))

par.names<-c(level1.par.names,level2.par.names)
n.pars=length(par.names)

n.chains=24
nmc=5000
burnin=1000
thin=1
keep.samples=seq(burnin,nmc,thin)
print(length(keep.samples)*n.chains)


use.optim=TRUE
optim.gamma=TRUE
migrate.prob=.1
migrate.duration=round(burnin*.25)+1
b=.001

cores=8
data<-data[seq(1,161,10)]
S=length(data)


#defining initial values
#because this time, these variables will be normal distributions, means can be zero, SDs can be ones.

#MAY NEED TO ADJUST THESE TAU VALUES IN LIGHT OF PUTTING THIS IN NORMAL DISTRIBUTION SPACE
x.init<- list()
x.init[[level1.par.names[[1]]]]<-rep(0,S)#alpha
x.init[[level1.par.names[[2]]]]<-rep(2,S)#thresh
x.init[[level1.par.names[3]]]=.6*(sapply(data,function(x)min(x$rt,na.rm=TRUE))) #is this the best we can do?
#mu values
for (pn in level2.par.names[1:2]){
  x.init[[pn]]<-0
}
x.init[[level2.par.names[3]]]=.6*mean(sapply(data,function(x)min(x$rt,na.rm=TRUE))) #is this the best we can do?
#sigma values
for (pn in level2.par.names[4:5]){
  x.init[[pn]]<-1
}
x.init[[level2.par.names[6]]]=.6*sd(sapply(data,function(x)min(x$rt,na.rm=TRUE))) #is this the best we can do?

n.parinstances<-length(unlist(x.init))#the list of parameter instances, counting each element of the vector parameters
##############################################  set prior

prior=NULL
# upper and lower boundaries for the concentration parameters
prior$lower=0  
prior$upper=1

########################################## run it
source(paste0("de_mcmc/model_configs/de_",version,"_config.R"))
source(paste0("de_mcmc/de_",version,"_run.R"))


#######################################################################################################################################
#CALL run_env<-de_mcmc_execute(log.dens.like.h.m1,log.dens.prior.h.m1)#log.dens.like.f<-log.dens.like.h.m1;log.dens.prior.f<-log.dens.prior.h.m1
#######################################################################################################################################
#run_env<-de_mcmc_execute(log.dens.like.h.m1,log.dens.prior.h.m1)#log.dens.like.f<-log.dens.like.h.m1;log.dens.prior.f<-log.dens.prior.h.m1
#log.dens.like.f<-log.dens.like.h.m1;log.dens.prior.f<-log.dens.prior.h.m1
mainDir<-getwd()
subDir=""

sfInit(parallel=TRUE, cpus=cores, type="SOCK")
#printv("setting up cluster...")
sfClusterSetupRNG()

ptm=proc.time()[3]
#printv ("running the model...")
use.data<-data
#######################################################################################################################################
#call de_m1_run(log.dens.like.f=log.dens.like.f,
#               log.dens.prior.f=log.dens.prior.f)
log.dens.like.f<-log.dens.like.h.m1;log.dens.prior.f<-log.dens.prior.h.m1
#######################################################################################################################################

log.dens.post=function(x,use.data,prior)log.dens.prior.f(x,prior) + log.dens.like.f(x,use.data)

########################################### initialize the chains
printv("intializing chains...")
theta<<-array(NA,c(n.chains,n.pars,S))
weight<<-array(-Inf,c(n.chains,S))

colnames(theta) <- par.names
printv("exporting...")
sfExportAll(except=list("theta","weight"))

print("Optimizing...(this may take some time)")
init.pars=matrix(1,n.pars)
#I don't look through becasue we no longer have a single matrix of everything
#we have a list, some of which are across-subject values, some of which are not.
x=unlist(x.init)
library(profvis)
#profvis({temp.weight=log.dens.like.f(x,use.data=data)})

new.x=x
#while(temp.weight==-Inf){
  #stop("I think this hasn't been set up properly. Not only are the values probably wrong but the function takes far too long to run!")
  #is this appropriate for all the variables? I'm not sure.
  #new.x=rtnorm(n.parinstances,.1,0,inf.vals) #BJS to BT: why does this use a truncated normal distribution?
  #warning("We use this truncated normal distribution to estimate new x values, but I'm not sure that this is appropriate for all variables.")
  
  #shouldn't we be getting the priors here?
  #temp.weight=log.dens.like.f(new.x,use.data=data)
profvis({
#}
#######################################################################################################################################
#log.dens.like.h.m1<-function(x,use.data){
#######################################################################################################################################
  log.dens.like.h.m1.parnames.alpha<-NULL
  log.dens.like.h.m1.parnames.thresh<-NULL
  log.dens.like.h.m1.parnames.tau<-NULL
    cat("*")
    if(is.null(log.dens.like.h.m1.parnames.alpha)){
      #do this here to speed up processing. Should only be one ONCE.
      log.dens.like.h.m1.parnames.alpha<<-lapply(1:length(use.data),function(s){paste0("alpha_s",s)})
      log.dens.like.h.m1.parnames.thresh<<-lapply(1:length(use.data),function(s){paste0("thresh_s",s)})
      log.dens.like.h.m1.parnames.tau<<-lapply(1:length(use.data),function(s){paste0("tau_s",s)})
    }
    #model
    dens=0
    if(check.pars(x=x,use.data=use.data)){
      #iterate through all trials.
      #one approach is to write a separate function
      for (s in 1:length(use.data)){#s<-1#first subject
        nt=length(use.data[[s]]$choice)
        #print(paste0("processing sub ",s))
        #100 is the number of slots we have to store cues (images), not individual trials.
        #since we don't record a new EV value each time.
        ev=matrix(0,100,2)
        #record the values at each time point.
        v_t=matrix(0,nt,2)
        for(tr in 1:nt){#tr=1;tr=tr+1
          #print (tr)
          #start_time <- Sys.time()
          if (use.data[[s]]$choice[tr]!=0) {
            
            #this must come first - this represents the choice being made.
            # there is some transformation based on ev and beta needed before a drift rate can be obtained
            v_t[tr,]=invlogit(ev[use.data[[s]]$cue[tr],])
            
            # prediction error
            PE   =  use.data[[s]]$outcome[tr] - ev[use.data[[s]]$cue[tr],use.data[[s]]$choice[tr]]
            PEnc = -use.data[[s]]$outcome[tr] - ev[use.data[[s]]$cue[tr],3-use.data[[s]]$choice[tr]]
            
            # value updating (learning)
            ev[use.data[[s]]$cue[tr],3-use.data[[s]]$choice[tr]] = 
              ev[use.data[[s]]$cue[tr],3-use.data[[s]]$choice[tr]] + as.numeric(x[[log.dens.like.h.m1.parnames.alpha[[s]] ]]) * PEnc;
            ev[use.data[[s]]$cue[tr],use.data[[s]]$choice[tr]] = 
              ev[use.data[[s]]$cue[tr],use.data[[s]]$choice[tr]] + as.numeric(x[[log.dens.like.h.m1.parnames.alpha[[s]] ]]) * PE;
            
          }
        }
          #end_time <- Sys.time()
          #log.dens.like.h.m1.timer.sectionA<<-log.dens.like.h.m1.timer.sectionA+(end_time-start_time)
          
          #warning("Does it make sense to get the log of the likelihood for each single trial then sum that together?")
          #warning("Does it make sense to never get likelihood data based on the double update model, and only the racing diffusion model?")
          #start_time <- Sys.time()
          
          #save(list(list(use.data),list(x),list(v_t)), file=paste0(dd, "test_de_h_m1_config.Rdata"))
          #stop("stopping here so I can use profvis well.")
          #######################################################################################################################################
          # dens=dens+sum(log(get.dens.2choice(t=use.data[[s]]$rt[use.data[[s]]$choice!=0],
          #                                    choice=use.data[[s]]$choice[use.data[[s]]$choice!=0],
          #                                    alpha=c(x[[paste0("thresh_s",s)]],x[[paste0("thresh_s",s)]]),
          #                                    v=v_t[use.data[[s]]$choice!=0,],
          #                                    theta=c(x[[paste0("tau_s",s)]],x[[paste0("tau_s",s)]])
          t=use.data[[s]]$rt[use.data[[s]]$choice!=0]
          choice=use.data[[s]]$choice[use.data[[s]]$choice!=0]
          alpha=c(x[[log.dens.like.h.m1.parnames.thresh[[s]] ]],x[[log.dens.like.h.m1.parnames.thresh[[s]] ]])
          v=v_t[use.data[[s]]$choice!=0,]
          theta=c(x[[log.dens.like.h.m1.parnames.tau[[s]] ]],x[[log.dens.like.h.m1.parnames.tau[[s]] ]])
          # )))
          #######################################################################################################################################
          
          idx1=(choice==1)
          tmp=numeric(length(idx1))
          #tstart<-Sys.time()
          
          #x_w=t[idx]-theta
          #tmp[idx1]=wald.pdf.raw(t[idx1],alpha[1],v[idx1,1],theta[1])*(1-wald.cdf.raw(t[idx1],alpha[2],v[idx1,2],theta[2]))
          profvis({for (i in 1:10000){
            theta[1]<-runif(1,0.2,0.5)
            tmp[idx1]=wald.pdf.c(t[idx1],alpha[1],v[idx1,1],theta[1])*(1-wald.cdf.c(t[idx1],alpha[2],v[idx1,2],theta[2]))}})
          profvis({for (i in 1:10000){
            theta[1]<-runif(1,0.2,0.5)
            tmp[idx1]=wald.pdf.raw(t[idx1],alpha[1],v[idx1,1],theta[1])*(1-wald.cdf.raw(t[idx1],alpha[2],v[idx1,2],theta[2]))}})
          
          idx2=(choice==2)
          
          #tmp[idx2]=wald.pdf.raw(t[idx2],alpha[2],v[idx2,2],theta[2])*(1-wald.cdf.raw(t[idx2],alpha[1],v[idx2,1],theta[1]))
          tmp[idx2]=wald.pdf.raw(t[idx2],alpha[2],v[idx2,2],theta[2])*(1-wald.cdf.raw(t[idx2],alpha[1],v[idx2,1],theta[1]))
          get.dens.2choice_count<<-get.dens.2choice_count+1
          #tend<-Sys.time()
          #time.wald<<-time.wald+(tend-tstart)
          #if((get.dens.2choice_count%%1000)==0) print(paste0(get.dens.2choice_count, "twalds:",time.wald))
          tmp
          #end_time <- Sys.time()
          #log.dens.like.h.m1.timer.sectionB<<-log.dens.like.h.m1.timer.sectionB+(end_time-start_time)
        }
        #printv("running getting density")
        # now pass the matrix of v into the density wrapper, where RT and choice are vectors.
        #we use this to calculate the density associated with the model
        #
      
      #what about the density for the group-level variables?
      #I think we can just do density for the subject-level variables; and since they are being fit from the group-level,
      #that in itself tells us how close we're getting.
      #we have no group-level 'data' to fit so it wouldn't make sense to get any density for hte group level.
      
      #printv("...got density.")
      #print(paste0("A:",log.dens.like.h.m1.timer.sectionA,"; B:",log.dens.like.h.m1.timer.sectionB))
      
      out=dens
      if(is.na(out))out=-Inf
    } else {
      out=-Inf
    }
    out
  

})


#if(use.optim==FALSE)init.pars=new.x
#print(paste("Optimization Complete."))
#}
