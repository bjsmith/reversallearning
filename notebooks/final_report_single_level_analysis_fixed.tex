\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Final Report: Single level analysis},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Final Report: Single level analysis}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  

\begin{document}
\maketitle

I ran several single-level joint analyses. These ran quickly without the
limitations I experienced running multi-level analyses for joint models.

\subsection{Limitations of joint
models.}\label{limitations-of-joint-models.}

Both the Differential Evolution joint models and the stan NUTS models
could not proceed. The Differential Evolution model was able to produce
estimates using a two-level model. However, it was unstable in producing
estimates at three levels.

\% Remember? this was when I ran some massive, 10,000 iteration models;
they seemd to run but they were messy and Brandon said the chains were
unacceptably all over the place, which they kind of were.

\subsubsection{Experimenting with
initialization}\label{experimenting-with-initialization}

The three level NUTS model would initialize, run, and produce an
estimate for small groups of as many as 15 subjects but could not
properly intialize for larger groups. Because initializing seemed to be
the main problem, I focused a lot on attempting to resolve
initialization problems. The rstan MCMC estimator works by starting
chains off at a particular point within the prior distribution, and
calculates Jacobian transforms away from that point repeatedly, picking
more likely proposals with a randomly determined probability. THus, it
is important that the estimator starts at a point in the probability
space that it can iterate from. If the point is too far at one extreme
of the distribution, then the model may fail to properly intialize.

I thus experimented with several initialization methods, including
randomization and bootstrap methods. For more information on these, see
hierarchical\_noncentered\_compare\_starting\_values.

\section{Single-level models}\label{single-level-models}

With single-level models, it was easier to quickly run models and also
to implement joint models. In single-level reversal learning models, I
experimented with:

\begin{itemize}
\tightlist
\item
  Different behavioral models, including a discrimination model, an
  OPAL-inspired model, a simple reinforcement learning model, and a LBA
  RL model.
\item
  centered and non-centered parameterizations
\item
  Variation of priors
\item
  Variation of covariance matrix estimation method
\end{itemize}

NEED TO EXPLAIN EACH OF THESE!

\subsection{Single-level behavioral LBA RL
model}\label{single-level-behavioral-lba-rl-model}

Overall, I found that the behavioral LBA RL model was the best choice. I
used a non-centered parameterization, and kept to priors I had been
previously using, with a few exceptions.

\subsubsection{Model}\label{model}

The model implemented Annis, Palmeri, Miller, as discussed elsewhere.

\subsubsection{Prior distributions}\label{prior-distributions}

In order to facilitate ease of future development of the model into a
hierarchical model, at all times I attempted to generate priors using
conjugate distributions. Thus, parameter priors were all initially
estimated on a normal distribution before being transformed into a more
appropriate distribution. The \(\alpha\) learning parameter is on a
scale from 0 to 1, where zero represents no learning, and 1 represents a
perfectly learned stimulus-response pair on each trial. An inverse logit
transform converts \(\alpha\) from the normal distribution into a logit
distribution. At first, I had erroneously used large sigma values on the
normal distribution with an aim of producing a non-informative prior.
However, sigma values of greater than about 1.7 actually manipulate the
shape of the alpha distribution to produce higher probabilities near the
0 and 1 point and lower probabilities at the midpoint. Thus, I settled
on a prior for \(\alpha\) of 1.5.

\(k\) and \(\tau\) were both estimated by starting with normal priors
and were transformed to an exponential distribution with the inverse
natural log function \(e^x\).

(we have the runs for this; consider adding them in!)

I followed recommended stan best practice ( need to add reference to
this; I have done some writing on this already!) to use the following
process for creating a covariance matrix:

\begin{itemize}
\tightlist
\item
  Generate a lower-half Omega matrix using an LKJ cholesky correlation
  matrix function with an \(\eta\) parameter of 1.
\item
  Generate a lower-half sigma matrix from a cauchy distribution with a
  \(\mu=0, \sigma=1\), equal to the number of linked parameters.
\item
  Get the product of the diagonal matrix formed from the lower-half
  sigma value and the lower-half Omega matrix.
\item
  Generate a Sigma matrix using the product of the lower-half sigma
  matrix and its transpose.
\item
  Normalize each linked parameter by subtracting its mean and dividing
  the parameter by its standard deviation.
\item
  Estimate the log probability of the normalized parameter matrix given
  the Sigma matrix.
\end{itemize}

The normalized sigma matrix is effectively a correlation matrix.

I sampled from the prior distribution of the matrix derived from this
method and confirmed that although not completely uninformative, with
some bias toward zero covariance, priors were loose and enabled wide
ranges of proposal covariance values to be considered.

\subsubsection{Regions}\label{regions}

I tried analyzing freesurfer-derived regions as well as FSL-derived
regions. There were adequately large correlations between freesurfer
subcortical regions and FSL-derived regions to suggest they were
capturing roughly the same regions. Correspondence between cortical
regions could not be determined because each method--Freesurfer and
FSL--subdivide the cortex using very different regional boundaries.

\subsection{Results}\label{results}

For the first model, I ran the model examining potential links with 25
selected freesurfer regions identified as part of the decision-making
network.

68 runs did not complete. Runs fail to complete for several reasons,
including timing out after 2 hours of failing to converge, or because
\(\hat{R}\) values are insufficiently close to 1, indicating poor
convergence. There were 7 subjects missing from the analysis in the
analysis altogether.

\includegraphics{final_report_single_level_analysis_fixed_files/figure-latex/Resultsv11f_table_byRPE-1.pdf}

The Putamen correlated most strongly with the Reward Prediction Error.
The Accumbens, cingulate gyrus (both ACC and PCG), and caudate
correlated, though less strongly.

\includegraphics{final_report_single_level_analysis_fixed_files/figure-latex/Resultsv11e_table_byEV-1.pdf}

Expected value not strongly linked to anything, but there was a weak,
significant relationship with the frontal superior sulcus and putamen.
There was a negative relationship betewen EV and regions right across
theleft and right insula (i.e., low expected values were associated with
higher insula activity).

Suborbital prefrontal regions did not show the expected effects. The
suborbital sulcus was linked \emph{negatively} to expected value, and
several other relevant areas - in particular, the rectus gyrus, did not
significantly correlate with the modeled expected value signal.

ADD: count of subjects included in this analysis.

\subsection{Targeted analyses}\label{targeted-analyses}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"/expdata/bensmith/joint-modeling/data/msm/reversallearning/lba_rl/joint_20180917_1/lba_rl_single_exp_joint_v11t_rsdt.RData"}\NormalTok{)}
\NormalTok{runs_missing_data <-}\StringTok{ }\KeywordTok{runs_missing}\NormalTok{(rawdata, rsdt, }\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

I re-ran the single-level model to test whether we could get a stronger
covariance result by focusing on a few regions. Using the LJK estimator,
the estimates do covary between cells across samples and so it might be
important to only include the few values we really think will show a
correlation. In this analysis, 112 runs were missing and 9 subjects had
no runs at all. The missing runs are a concern, and I believe that these
missing runs could be driving the problems I have been having getting
three-level models running.

\subsection{Correlation size and number of
parameters}\label{correlation-size-and-number-of-parameters}

These tiny correlation estimates don't seem to be related to the number
of parameters. When I run a model with just a Delta parameters, the
sizes are no larger.

\includegraphics{final_report_single_level_analysis_fixed_files/figure-latex/JustDeltaParams1-1.pdf}

The results are significant for reward prediction error, but not for
expected value, consistent with the previous test. Both regions, the
suborbital sulcus and accumbens, are positively associated with reward
prediction error, while neither are significantly related to the
Accumbens. This is a partial confirmation of the hypothesis that the
Accumbens would be associated with reward prediction error while the
suborbital sulcus would be associated with expected value.


\end{document}
