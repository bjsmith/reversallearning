---
title: "Testing Punishment and reward effect"
output: html_notebook
---

#Motivation

In previous testing (see test_punishment_reward_effect.Rmd), I found that the multi-motivation, multi-run double-update model wasn't correctly recording beta values for the Punishment subjects.

I wondered if there was a bug in the stan file itself, and so to try and reduce chances for errors, I reprogrammed the stan file so that reward and punishment would be treated exactly the same, using loops that run through to execute precisely the same lines of code for both reward and punishment.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE)
knitr::opts_knit$set(root.dir="../")

```

```{r setup2, include=FALSE}
source("util/apply_local_settings.R")
source("visualization/geom_hdi.R")
apply_local_settings()
knitr::opts_chunk$set(cache.path = paste0(localsettings$data.dir,"knitrcache"))
source("nate_files/fitGroupsV3Onegroup.R")
source("data_summarize.R")
library(data.table)
library(ggplot2)
```


# Method


```{r model_run_prep, echo=TRUE, message=FALSE, warning=FALSE}
source("du_model_otloop.R")
```

The Double Update Model loop was 

In order to run this test successfully we need to:
 - present the previous effect, showing a large difference in inverse temperature between reward and punishment groups. 
 - For affirmative results--confirming the difference is not due to a misconfiguration of the larger model--we need to show that parameters for individually-calculated groups are similar.
 - For negative results--confirming there is a problem with the larger model--we need to show that parameters for the individually-calculated groups do not differ to the same degree by group.

```{r setup3, message=FALSE, warning=FALSE}

#arrange all the data into a single data table.
model.summary.all<-NULL

#iterations
miters<-unlist(lapply(model.summaries,function(m){
  return(length(m$summaryObj$iter))
}))
for(ms.i in 1:length(model.summaries)){
  #i=2
  ms<-model.summaries[[ms.i]]
  ms.summaryObj<-ms$summaryObj
  ms.summaryObj$TestId<-ms.i
  ms.summaryObj$Group<-ms$g
  ms.summaryObj$ModelName<-ms$m
  ms.summaryObj$AnalysisRepetition<-ms$t
  ms.summaryObj$EstimationMethod<-ms$EstimationMethod
  if(is.null(model.summary.all)){
    model.summary.all<-ms.summaryObj
  }else{
    model.summary.all<-rbind(model.summary.all,ms.summaryObj)
  }
}
model.summary.all$EstimationMethod<-factor(model.summary.all$EstimationMethod)

```


```{r data table, message=FALSE, warning=FALSE}

ms.summary<-NULL

for (i in 1:length(model.summaries)){
  ms<-model.summaries[[i]]
  dt.i<-data.table(
    "Group"=ms$g,
    "AnalysisRepetition"=ms$t,
    "Model"=ms$m,
    "EstimationMethod"=ms$EstimationMethod,
    "Duration"=ms$elapsedTime
    )
  if(dim(dt.i)[1]>1){
    print("dt greater than 1")
    print(i)
    print(summary(ms))
    
    next
  }
  
  if(is.null(ms.summary)){
    ms.summary<-dt.i
    
  }else{
    ms.summary<-rbind(ms.summary,dt.i)
  }
}

knitr::kable(ms.summary[,.N,by=.(EstimationMethod,Model,Group)])

```


# Results

First, the effect we want to demonstrate in parameters in separately-estimated groups, and do a sanity check that results don't differ materially across analysis repetitions:

```{r sanity check}
m.combined.analysisrepcheck<-model.summary.all[EstimationMethod=="MCMC" & ModelName=="double_update_rpo_repeated_runs_notrialpost" &Parameter=="beta"]

ggplot(m.combined.analysisrepcheck,aes(x=Value,fill=factor(AnalysisRepetition),color=factor(AnalysisRepetition)))+
    geom_freqpoly(alpha=0.5,binwidth=0.001)+
     geom_hdi(size=2, lineend = "round",alpha=0.2,credible_mass=0.95)+
#    coord_cartesian(ylim=c(0,80))+
    facet_grid(Run+Statistic~Motivation+Group,scales="free")+
    labs(title=expression(paste(alpha[mu], " and ", beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))

model.summary.ar1<-model.summary.all[AnalysisRepetition==1]
```
Results are very consistent across repetitions, so for teh remainder of this analysis, we'll only look at the first.

```{r inverse temperature difference}

m.combined<-model.summary.ar1[EstimationMethod=="MCMC" & ModelName=="double_update_rpo_repeated_runs_notrialpost"]

ggplot(m.combined,aes(x=Value,fill=factor(Motivation),color=factor(Motivation)))+
    geom_freqpoly(alpha=0.5,binwidth=0.001)+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
#    coord_cartesian(ylim=c(0,80))+
    facet_grid(Run+Statistic~Parameter+Group,scales="free")+
    labs(title=expression(paste(alpha[mu], " and ", beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))
```


Does the method using loops rather than separate commands to calculate outcome type outcomes look similar or different?

```{r inverse temperature difference in separately estimated groups, echo=FALSE}

m.looped<-model.summary.ar1[EstimationMethod=="MCMC" & ModelName=="double_update_rpo_repeated_runs_ntp_otmod"]

ggplot(m.looped,aes(x=Value,fill=factor(Motivation),color=factor(Motivation)))+
    geom_freqpoly(alpha=0.5,binwidth=0.001)+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
#    coord_cartesian(ylim=c(0,80))+
    facet_grid(Run+Statistic~Parameter+Group,scales="free")+
    labs(title=expression(paste(alpha[mu], " and ", beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model using loop for Outcome Type")))
```

These models are very similar. what about for the second analysis repetition?

```{r}
m.separate.ar2<-model.summary.all[EstimationMethod=="MCMC" & ModelName=="double_update_rpo_repeated_runs_ntp_otmod" & 
                                    AnalysisRepetition==2]
ggplot(m.separate.ar2,
       aes(x=Value,fill=factor(Motivation),color=factor(Motivation))
       )+
     geom_freqpoly(alpha=0.5,binwidth=0.001)+xlim(0,3)+
  facet_grid(Run+Statistic~Group+Parameter,scales="free")+
    coord_cartesian(ylim=c(0,150),xlim=c(0,2))+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
     labs(title=expression(paste(beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))

m.combined.ar2<-model.summary.all[EstimationMethod=="MCMC" & ModelName=="double_update_rpo_repeated_runs_notrialpost" & 
                                    AnalysisRepetition==2]
hist(m.combined.ar2$Value[sample(0:length(m.combined.ar2$Value),1000)])

ggplot(m.combined.ar2,aes(x=Value,fill=factor(Motivation),color=factor(Motivation)))+
    geom_freqpoly(alpha=0.5,binwidth=0.001)+xlim(0,3)+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
#    coord_cartesian(ylim=c(0,80))+
    facet_grid(Run+Statistic~Parameter+Group,scales="free")+
    labs(title=expression(paste(alpha[mu], " and ", beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))
```

So it seems like the abberant Punishment results are not due to outcome type being treated differently in the code; these are now treated in precisely the same way.

Next thing to test might be swapping the data around - swap around the outcome types and see if that changes how the datasets are treated in the model.

If we do that, and the results come out still with the second factor abberant, that would suggest that the problem is in the way the code treats the second factor--whichever factor is numbered (2). If we do that and the results come out with the first factor abberrant, that would then suggest there's some characteristic of the data itself that causes the model to interact with in this way. It could still be a misspecification of the model.


# Variational Bayes

This has been taking a long time. I want to see whether we can run the same tests using variational Bayes. Does Variational Bayes estimates yield the same problem? If it does, we can use vb to diagnose the problem much more quickly.


```{r model_run_prep_vb}
source("du_model_otloop_vb.R")
```



```{r setup3vb, message=FALSE, warning=FALSE}

#arrange all the data into a single data table.
model.summary.all<-NULL

#iterations
miters<-unlist(lapply(model.summaries,function(m){
  return(length(m$summaryObj$iter))
}))
for(ms.i in 1:length(model.summaries)){
  #i=2
  ms<-model.summaries[[ms.i]]
  ms.summaryObj<-ms$summaryObj
  ms.summaryObj$TestId<-ms.i
  ms.summaryObj$Group<-ms$g
  ms.summaryObj$ModelName<-ms$m
  ms.summaryObj$AnalysisRepetition<-ms$t
  ms.summaryObj$EstimationMethod<-ms$EstimationMethod
  if(is.null(model.summary.all)){
    model.summary.all<-ms.summaryObj
  }else{
    model.summary.all<-rbind(model.summary.all,ms.summaryObj)
  }
}
model.summary.all$EstimationMethod<-factor(model.summary.all$EstimationMethod)


ms.summary<-NULL

for (i in 1:length(model.summaries)){
  ms<-model.summaries[[i]]
  dt.i<-data.table(
    "Group"=ms$g,
    "AnalysisRepetition"=ms$t,
    "Model"=ms$m,
    "EstimationMethod"=ms$EstimationMethod,
    "Duration"=ms$elapsedTime
    )
  if(dim(dt.i)[1]>1){
    print("dt greater than 1")
    print(i)
    print(summary(ms))
    
    next
  }
  
  if(is.null(ms.summary)){
    ms.summary<-dt.i
    
  }else{
    ms.summary<-rbind(ms.summary,dt.i)
  }
}

```


Table:


```{r}

knitr::kable(ms.summary[,.N,by=.(EstimationMethod,Model,Group)])

```

What do these repetitions look like?

```{r VariationalBayesRepetitions}
m.combined.analysisrepcheck<-model.summary.all[ ModelName=="double_update_rpo_repeated_runs_notrialpost" &Parameter=="beta"]

ggplot(m.combined.analysisrepcheck,aes(x=Value,fill=factor(AnalysisRepetition),color=factor(AnalysisRepetition)))+
    geom_freqpoly(alpha=0.5,binwidth=0.001)+
     geom_hdi(size=2, lineend = "round",alpha=0.2,credible_mass=0.95)+
#    coord_cartesian(ylim=c(0,80))+
    facet_grid(Run+Statistic~Motivation+Group,scales="free")+
    labs(title=expression(paste(alpha[mu], " and ", beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))


```



we'll only look at the first.

```{r inverse temperature difference vb}

m.combined<-model.summary.all[ModelName=="double_update_rpo_repeated_runs_notrialpost"]

ggplot(m.combined,aes(x=Value,fill=factor(Motivation),color=factor(Motivation)))+
    geom_freqpoly(alpha=0.5,binwidth=0.001)+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
#    coord_cartesian(ylim=c(0,80))+
    facet_grid(Run+Statistic~Parameter+Group,scales="free")+
    labs(title=expression(paste(alpha[mu], " and ", beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))
```


Yes: these appear to be essentially the same results as we got when using MCMC; in that the same error is apparent.

And do we also see it in VB for teh looped version?


```{r inverse temperature difference vb}

m.looped<-model.summary.all[ModelName=="double_update_rpo_repeated_runs_ntp_otmod"]

ggplot(m.looped,aes(x=Value,fill=factor(Motivation),color=factor(Motivation)))+
    geom_freqpoly(alpha=0.5,binwidth=0.001)+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
#    coord_cartesian(ylim=c(0,80))+
    facet_grid(Run+Statistic~Parameter+Group,scales="free")+
    labs(title=expression(paste(alpha[mu], " and ", beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))
```


Yeah, we do. So what does this mean?

First, Variational Bayes with 12 repetitions might be a good quick-and-dirty substitute for MCMC, for this model. It's still not quite as good as MCMC and shouldn't be an end-goal substittute, but we'll speed up testing a lot with it.

Second...I don't know what's wrong with the model. I've checked just about everything and ruled out all the problems with it that I can think of.