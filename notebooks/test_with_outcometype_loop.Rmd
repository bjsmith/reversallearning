---
title: "Testing Punishment and reward effect"
output: html_notebook
---

#Motivation

In previous testing (see test_punishment_reward_effect.Rmd), I found that the multi-motivation, multi-run double-update model wasn't correctly recording beta values for the Punishment subjects.

I wondered if there was a bug in the stan file itself, and so to try and reduce chances for errors, I reprogrammed the stan file so that reward and punishment would be treated exactly the same, using loops that run through to execute precisely the same lines of code for both reward and punishment.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=TRUE)
knitr::opts_knit$set(root.dir="../")

```

```{r setup2, include=FALSE}
source("util/apply_local_settings.R")
source("visualization/geom_hdi.R")
apply_local_settings()
knitr::opts_chunk$set(cache.path = paste0(localsettings$data.dir,"knitrcache"))
source("nate_files/fitGroupsV3Onegroup.R")
source("data_summarize.R")
library(data.table)
library(ggplot2)
```


# Method


```{r model_run_prep, echo=TRUE, message=FALSE, warning=FALSE}
source("du_model_otloop.R")
```

The Double Update Model loop was 

In order to run this test successfully we need to:
 - present the previous effect, showing a large difference in inverse temperature between reward and punishment groups. 
 - For affirmative results--confirming the difference is not due to a misconfiguration of the larger model--we need to show that parameters for individually-calculated groups are similar.
 - For negative results--confirming there is a problem with the larger model--we need to show that parameters for the individually-calculated groups do not differ to the same degree by group.

```{r setup3, message=FALSE, warning=FALSE}

#arrange all the data into a single data table.
model.summary.all<-NULL

#iterations
miters<-unlist(lapply(model.summaries,function(m){
  return(length(m$summaryObj$iter))
}))
for(ms.i in 1:length(model.summaries)){
  #i=2
  ms<-model.summaries[[ms.i]]
  ms.summaryObj<-ms$summaryObj
  ms.summaryObj$TestId<-ms.i
  ms.summaryObj$Group<-ms$g
  ms.summaryObj$ModelName<-ms$m
  ms.summaryObj$AnalysisRepetition<-ms$t
  ms.summaryObj$EstimationMethod<-ms$EstimationMethod
  if(is.null(model.summary.all)){
    model.summary.all<-ms.summaryObj
  }else{
    model.summary.all<-rbind(model.summary.all,ms.summaryObj)
  }
}
model.summary.all$EstimationMethod<-factor(model.summary.all$EstimationMethod)

```


```{r data table, message=FALSE, warning=FALSE}

ms.summary<-NULL

for (i in 1:length(model.summaries)){
  ms<-model.summaries[[i]]
  dt.i<-data.table(
    "Group"=ms$g,
    "AnalysisRepetition"=ms$t,
    "Model"=ms$m,
    "EstimationMethod"=ms$EstimationMethod,
    "Duration"=ms$elapsedTime
    )
  if(dim(dt.i)[1]>1){
    print("dt greater than 1")
    print(i)
    print(summary(ms))
    
    next
  }
  
  if(is.null(ms.summary)){
    ms.summary<-dt.i
    
  }else{
    ms.summary<-rbind(ms.summary,dt.i)
  }
}

knitr::kable(ms.summary[,.N,by=.(EstimationMethod,Model,Group)])

```


# Results

First, the effect we want to demonstrate in parameters in separately-estimated groups, and do a sanity check that results don't differ materially across analysis repetitions:

```{r sanity check}
m.combined.analysisrepcheck<-model.summary.all[EstimationMethod=="MCMC" & ModelName=="double_update_rpo_repeated_runs_notrialpost" &Parameter=="beta"]

ggplot(m.combined.analysisrepcheck,aes(x=Value,fill=factor(AnalysisRepetition),color=factor(AnalysisRepetition)))+
    geom_freqpoly(alpha=0.5,binwidth=0.001)+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
#    coord_cartesian(ylim=c(0,80))+
    facet_grid(Run+Statistic~Motivation+Group,scales="free")+
    labs(title=expression(paste(alpha[mu], " and ", beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))

model.summary.ar1<-model.summary.all[AnalysisRepetition==1]
```
Results are very consistent across repetitions, so for teh remainder of this analysis, we'll only look at the first.

```{r inverse temperature difference}

m.combined<-model.summary.ar1[EstimationMethod=="MCMC" & ModelName=="double_update_rpo_repeated_runs_notrialpost"]

ggplot(m.combined,aes(x=Value,fill=factor(Motivation),color=factor(Motivation)))+
    geom_freqpoly(alpha=0.5,binwidth=0.001)+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
#    coord_cartesian(ylim=c(0,80))+
    facet_grid(Run+Statistic~Parameter+Group,scales="free")+
    labs(title=expression(paste(alpha[mu], " and ", beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))
```


Do our separately-estimated group parameters appear similar?

```{r inverse temperature difference in separately estimated groups, echo=FALSE}

m.separate<-model.summary.ar1[EstimationMethod=="MCMC" & ModelName=="double_update_notrialpost"]

ggplot(m.separate,
       aes(x=Value,fill=factor(Motivation),color=factor(Motivation))
       )+
     geom_freqpoly(alpha=0.5,binwidth=0.001)+xlim(0,3)+
  facet_grid(Run+Statistic~Group+Parameter,scales="free")+
    coord_cartesian(ylim=c(0,150),xlim=c(0,2))+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
     labs(title=expression(paste(beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))

```


Unfortunately, these results indicate a problem with the model. When estimated separately, there is no difference between Reward and Punishment beta values. This likely indicates we have a problem in buliding our model, and in particular, it seems to be the Punishment values which are particularly problematic.

That isn't all, however. These results suggest that Run 2 contain some kind of irregularity. While parameter estimates for Run 1 appear reasonable, there seems to be a runaway chain for Run 2. That kind of data irregularity could potentially drive differences where the groups are estimated in one model together.

Does this appear for the second analysis repetition?

```{r}
m.separate.ar2<-model.summary.all[EstimationMethod=="MCMC" & ModelName=="double_update_notrialpost" & 
                                    AnalysisRepetition==2]
ggplot(m.separate.ar2,
       aes(x=Value,fill=factor(Motivation),color=factor(Motivation))
       )+
     geom_freqpoly(alpha=0.5,binwidth=0.001)+xlim(0,3)+
  facet_grid(Run+Statistic~Group+Parameter,scales="free")+
    coord_cartesian(ylim=c(0,150),xlim=c(0,2))+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
     labs(title=expression(paste(beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))

m.combined.ar2<-model.summary.all[EstimationMethod=="MCMC" & ModelName=="double_update_rpo_repeated_runs_notrialpost" & 
                                    AnalysisRepetition==2]
hist(m.combined.ar2$Value[sample(0:length(m.combined.ar2$Value),1000)])

ggplot(m.combined.ar2,aes(x=Value,fill=factor(Motivation),color=factor(Motivation)))+
    geom_freqpoly(alpha=0.5,binwidth=0.001)+xlim(0,3)+
     geom_hdi(size=2, lineend = "round",alpha=0.5,credible_mass=0.95)+
#    coord_cartesian(ylim=c(0,80))+
    facet_grid(Run+Statistic~Parameter+Group,scales="free")+
    labs(title=expression(paste(alpha[mu], " and ", beta[mu], " reward and punishment rounds\nfor DU Repeated Runs model")))
```


No, we don't get the same problems for the second repetition.

Need to look at what might be wrong:

 - Could be that we're not feeding data in tohte punishment trials right
 - Could be that we're not marking the types correctly in outcome_type
 - 


