---
title: "run single and multigroup double update models"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(rstan)
library(ggplot2)
```

# double update model, single groups


First, we want to try fitting the risky meth group. After that we can try fitting the risky non-meth group.

So far these are constrained to one run only each. The extra run would add extra complications. We need to add one additional complication at a time!

```{r double_update_risky_nonmeth_group, echo=TRUE}

source("fitGroupsV3OnegroupRun1.R")

fit<-lookupOrRunFit(run=1,2,model_to_use="double_update")

#try to get some info about the fit.
#mu_p[1] is estimated mean alpha [learning rate] across subjects
#mu_p[2] is estimated mean beta [inverse temperature]
#sigma[1] is estimated variance alpha
#sigma[2] is estimated variance beta
#Accessing the contents of a stanfit object
#https://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.html

fit.extracted<-rstan::extract(fit$fit)
#these parameters are not actually like the mean learning rate; 
#more like the 'intercept' in a linear model of the learning rate
#across subjects.

learning.rate.estimate<-data.table(mean=fit.extracted$mu_alpha,
                                   variance=fit.extracted$sigma[,1])
inverse.temperature.estimate<-data.table(mean=fit.extracted$mu_beta,
                                   variance=fit.extracted$sigma[,2])
ggplot(learning.rate.estimate,aes(x=mean,y=variance))+geom_point()
ggplot(inverse.temperature.estimate,aes(x=mean,y=variance))+geom_point()
```

Apparently we have negative learning rates and inverse temperatures here.

What about run 1, meth group?
```{r double_update_risky_meth_group, echo=TRUE}

source("../nate_files/fitGroupsV3OnegroupRun1.R")

fit<-lookupOrRunFit(run=1,3,model_to_use="double_update")

#try to get some info about the fit.
#mu_p[1] is estimated mean alpha [learning rate] across subjects
#mu_p[2] is estimated mean beta [inverse temperature]
#sigma[1] is estimated variance alpha
#sigma[2] is estimated variance beta
#Accessing the contents of a stanfit object
#https://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.html

fit.extracted<-rstan::extract(fit$fit)

learning.rate.estimate<-data.table(mean=fit.extracted$mu_alpha,
                                   variance=fit.extracted$sigma[,1])
inverse.temperature.estimate<-data.table(mean=fit.extracted$mu_beta,
                                   variance=fit.extracted$sigma[,2])
ggplot(learning.rate.estimate,aes(x=mean,y=variance))+geom_point()
ggplot(inverse.temperature.estimate,aes(x=mean,y=variance))+geom_point()

```


OK. now, we want to look into doing more than one group. Or more than one run.

Let's start with expanding to multiple groups, since I have already written the code for that!

```{r double_update_risky_meth_group, echo=TRUE}

source("../nate_files/fitGroupsV3OnegroupRun1.R")

fit<-lookupOrRunFit(run=1,c(2,3), model_to_use="prl_ben_v3_group",includeSubjGroup = TRUE)

#try to get some info about the fit.
#mu_p[1] is estimated mean alpha [learning rate] across subjects
#mu_p[2] is estimated mean beta [inverse temperature]
#sigma[1] is estimated variance alpha
#sigma[2] is estimated variance beta
#Accessing the contents of a stanfit object
#https://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.html

fit.extracted<-rstan::extract(fit$fit)

learning.rate.estimate<-data.table(mean=fit.extracted$mu_alpha,
                                   variance=fit.extracted$sigma[,1])
inverse.temperature.estimate<-data.table(mean=fit.extracted$mu_beta,
                                   variance=fit.extracted$sigma[,2])
ggplot(learning.rate.estimate,aes(x=mean,y=variance))+geom_point()
ggplot(inverse.temperature.estimate,aes(x=mean,y=variance))+geom_point()

```

OK, so we are getting convergence problems here. Nathan expects that when we try to estimate two independent groups at once.
So perhaps we should do each group separately? If we're doing that, then the next logical step is to take the double update model and try to add separate reward and punishment learning rates :-)

```{r double_update_risky_nometh_single_group_reward_and_punishment, echo=TRUE}
setwd("../nate_files/")
source("../nate_files/fitGroupsV3Onegroup.R")

#risky nometh
fit<-lookupOrRunFit(run=1,groups_to_fit=2, 
                    model_to_use="double_update_rp",
                    includeSubjGroup = FALSE,
                    rp=c(1,2),
                    model_rp_separately=TRUE,include_pain=FALSE)


#risky meth....

#try to get some info about the fit.
#mu_p[1] is estimated mean alpha [learning rate] across subjects
#mu_p[2] is estimated mean beta [inverse temperature]
#sigma[1] is estimated variance alpha
#sigma[2] is estimated variance beta
#Accessing the contents of a stanfit object
#https://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.html

fit.RiskyNoMeth.extracted<-rstan::extract(fit$fit)

learning.rate.estimate.g2<-rbind(data.table(mean=fit.RiskyNoMeth.extracted$mu_alpha_rew,
                                   variance=fit.RiskyNoMeth.extracted$sigma[,1,1],mode="reward",Group="RiskyNoMeth"),
                              data.table(mean=fit.RiskyNoMeth.extracted$mu_alpha_pun,
                                   variance=fit.RiskyNoMeth.extracted$sigma[,2,1],mode="punishment",Group="RiskyNoMeth"))

inverse.temperature.estimate.g2<-rbind(data.table(mean=fit.extracted$mu_beta_rew,
                                   variance=fit.extracted$sigma[,1,2],mode="reward",Group="RiskyNoMeth"),
                              data.table(mean=fit.extracted$mu_beta_pun,
                                   variance=fit.extracted$sigma[,2,2],mode="punishment",Group="RiskyNoMeth"))

fit.RiskyMeth<-lookupOrRunFit(run=1,groups_to_fit=3, model_to_use="double_update_rp",includeSubjGroup = FALSE,
                    rp=c(1,2),model_rp_separately=TRUE,include_pain=FALSE)

fit.RiskyMeth.extracted<-rstan::extract(fit.RiskyMeth$fit)

learning.rate.estimate<-rbind(learning.rate.estimate.g2,data.table(mean=fit.RiskyMeth.extracted$mu_alpha_rew,
                                   variance=fit.RiskyMeth.extracted$sigma[,1,1],mode="reward",Group="RiskyMeth"),
                              data.table(mean=fit.RiskyMeth.extracted$mu_alpha_pun,
                                   variance=fit.RiskyMeth.extracted$sigma[,2,1],mode="punishment",Group="RiskyMeth"))

inverse.temperature.estimate<-rbind(inverse.temperature.estimate.g2,data.table(mean=fit.RiskyMeth.extracted$mu_beta_rew,
                                   variance=fit.RiskyMeth.extracted$sigma[,1,2],mode="reward",Group="RiskyMeth"),
                              data.table(mean=fit.RiskyMeth.extracted$mu_beta_pun,
                                   variance=fit.RiskyMeth.extracted$sigma[,2,2],mode="punishment",Group="RiskyMeth"))

ggplot(learning.rate.estimate,aes(x=mean,y=variance,colour=Group))+geom_point()+
  facet_grid(.~mode)+
  labs(title="Estimated group-level learning rate and variance for reward and punishment\n(Run 1)")

ggplot(inverse.temperature.estimate,aes(x=mean,y=variance,colour=Group))+geom_point()+
  facet_grid(.~mode)+
  labs(title="Estimated group-level inverse temperature and variance for reward and punishment\n(Run 1")

#what would a confidence interval of these look like? We have 1000 samples, so we should calculate, for 1000 samples, what the 95% confidence interval is for the mean and variance parameters.

#it's arbitrary, but we can just combine for each sample in group 1 with each sample in group 2
#that would give us the confidence interval of the mean and variance parameter. I then need to think carefully about how these should be represented as confidence intervals of the population.
#so first: for mu_alpha_rew
group1.mu<-fit.RiskyNoMeth.extracted$mu_alpha_rew
group2.mu<-fit.RiskyMeth.extracted$mu_alpha_rew
mu.diff<-group2.mu-group1.mu#so in all possible samples the difference in groups in mean alpha parameters exists
#but what about the variance parameter? 
#We know that these two particular groups always differ, but does that really tell us that across the population, this is the same?
#What we have is samples representing the probability space of a normal distribution.
#We need to iterate through each sample and find out, for each sample, do the distributions differ 95% of the time?
#I might need to re-read the chapter from Kruschke to get a good intuition here because I don't know whether:
# - we can just take a difference in the group mean parameter to mean a population difference definitely exists...
# - or whether we need to do random sampling from the 1000 sample distributions or
# - whether we can use an analytical approach on the 1000 sample distributions
```

#Multiple runs

We have expanded the stan model to handle both reward and punishment parameters. But currently the model only handles one run.
We can expand the model to calculate for an arbitrary number of runs. We add a "run multiplier" parameter for each subject-level parameter and run.
The run multiplier is set to 1 for the first run, and can vary freely for subsequent runs.
There is an independent run multiplier parameter for each subject, for reward and punishment trials, for each parameter (alpha and beta) and each run after the first run.
At the group level, the run multiplier (except for the first) for each subject comes from a population of run multipliers.
There is a mean and variance of the population of run multipliers, just as there is for the population of alphas and betas.
Although our data only has up to two runs per subject, this does handle an arbitrary number of runs. However, there may be problems dealing with subjects who have fewer than the normal number of runs, and we might have to record the number of runs per subject in order to take that into acccount.

We could have modeled each run as its own separate parameter within a distribution of runs for each subject. However, it may not be appropriate to assume each run is sampled from a random distribution of runs for each subject, because order effects may mean that run 2 is qualitatively different to round 1. As such, our approach of using a difference parameter to estimate the difference between run 1 and run 2 can be appropriate.


The model is run as follows:

```{r double_update_reward_and_punishment_multiple_runs, echo=TRUE}
source("../")
source("notebooks/double_update_reward_and_punishment_multiple_runs.R")

#for each subject, subject-level mu and sigma alpha and beta for {reward,punishment}*{run1,run2}
#let's leave the trial-level values for now.
#so, let's have one data frame to represent group-level values
#and another data frame to represent subject-level values
#we will also need to add a column to express group level
#do it in long format; I think that'll make it easier to get dimensions; we can reshape as necessary.
ggplot(dcast(
  grouplevel.dt[Run==1],iter+Motivation+Group+Statistic~Parameter,
  value.var=c("Value"))
       ,aes(x=alpha,y=beta,colour=Group))+geom_point()+
  facet_grid(Motivation~Statistic)+
  labs(title="Posterior group-level learning rate and inverse temperature for reward and punishment\n(First Run)",
       x="alpha (Learning rate)",y="beta (inverse temperature)")


ggplot(dcast(
  grouplevel.dt[Run==1 & Group %in% c("RiskyNoMeth","RiskyMeth")],iter+Motivation+Group+Parameter~Statistic,
  value.var=c("Value"))
       ,aes(x=mu,y=sigma,colour=Group))+geom_point()+
  facet_grid(Motivation~Parameter,scales="free")+
  labs(title="Posterior group-level learning rate mu and sigma for reward and punishment\n(First Run)"
       )
# dres<-dcast(
#   grouplevel.dt[Run==1 & Parameter=="alpha"],iter+Motivation+Group+Statistic~Parameter,
#   value.var=c("Value"))
# 
# 
# ggplot(dcast(
#   grouplevel.dt[Run==1 & Parameter=="alpha"],iter+Motivation+Group~Value,
#   value.var=c("Value"))
#        ,aes(x=alpha,y=beta,colour=Group))+geom_point()+
#   facet_grid(Motivation~Statistic)+
#   labs(title="Posterior group-level learning rate and inverse temperature for reward and punishment\n(First Run)",
#        x="alpha (Learning rate)",y="beta (inverse temperature)")



```


So - we see pretty clear patterns here. Subjects appear to learn faster for Reward than Punishment, at least the RiskyMeth group, and faster for the RiskyNoMeth group than the RiskyMeth group.

However, variance is reasonably high, so although one group clearly has a lower average than the other, there is a lot of overlap between the groups.

Looking at the inverse temperature parameter, RiskyMeth subjects definitely have higher inverse temperatures. For both groups, inverse temperature is higher for reward than punishment. A higher inverse temperature indicates *more* consistency among responses - in other words, RiskyMeth subjects appear to be more *consistent* in their responses than RiskyNonMeth subjects.

## Speed
Can we speed this up at all? This takes a look at a model where reward and punishment have been vectorized. Hopefully, this will make things faster.

```{r Vectorized}
#first iteration.
fit.RiskyNoMeth.fast<-lookupOrRunFit(
  run=c(1,2),groups_to_fit=2, model_to_use="double_update_rpo_repeated_runs",includeSubjGroup = FALSE,
  rp=c(REVERSAL_LEARNING_REWARD,REVERSAL_LEARNING_PUNISHMENT),
  model_rp_separately=TRUE,model_runs_separately = TRUE, include_pain=FALSE,fastDebug = TRUE)
```
The "vectorized" version is likely not faster because, although we turned Reward and Punishment into separate variables, we weren't able to
actually vectorize any more operations. Operations like generating normal distributions in stan can only be done on vectors, not matrices or
n-D arrays, so there is little point in vectorizing.



Let's take a look at:
 * Whether subjects change their learning rate or inverse temperature from one run to the next.
 * Calculate HDIs for these averages
 * Get posterior predictions per subject
 * Describe the odds of an individual subject in the first group having a higher or lower learning rate 
 than an individual subject in the second goup. This would probably involve modifying the model's generated quantities to generate a sample of subjects.
 
 
#Learning rate ratios 

For learning rate ratios, see debugging_double_run_model.Rmd
```{r calc_hdi, echo=TRUE}


ggplot(dcast(
  grouplevel.dt[Statistic=="mu"],iter+Motivation+Group+Statistic+Run~Parameter,
  value.var=c("Value"))
       ,aes(x=alpha,y=beta,colour=Run))+geom_point()+
  facet_grid(Group~Motivation)+
  labs(title="Posterior group-level learning rate and inverse temperature for reward and punishment\n(First Run)",
       x="alpha (Learning rate)",y="beta (inverse temperature)")

```


#HDIs
```{r calc_hdi, echo=TRUE}
library(hBayesDM)
HDIofMCMC(fit.RiskyMeth.ex$mu_alpha_pun_run_multiplier[,1])
HDIofMCMC(fit.RiskyNoMeth.ex$mu_alpha_pun_run_multiplier[,1])

HDIofMCMC(fit.RiskyNoMeth.extracted$mu_beta_rew)
HDIofMCMC(fit.RiskyNoMeth.extracted$mu_alpha_rew)

#OK, so let's do some group comparisons...
#what is the difference in estimated mean learning rate between groups?

#so, let's reshape into a shape and then we can describe statistically
groupcomparison.dt<-dcast(
  grouplevel.dt[Statistic=="mu"],iter+Motivation+Statistic+Run~Parameter+Group,
  value.var=c("Value"))
# - Risky No Meth <> Risky Meth, mu, {reward, punishment}
#reshape to put data from each group onto the same line
#need one column per Group

hist(groupcomparison.dt$alpha_RiskyNoMeth-groupcomparison.dt$alpha_RiskyMeth)
#does RiskyNoMeth have a higher learning rate than RiskyMeth?

#overall
source("../visualization/geom_hdi.R")
p<-ggplot(groupcomparison.dt,aes(x=alpha_RiskyNoMeth-alpha_RiskyMeth))+geom_histogram(bins=30)+
  labs(title="Difference in posterior learning rate, NoMeth-Meth")+
  geom_hdi(size=4, lineend = "round",color="blue",alpha=0.5,credible_mass=0.95)
plotly::ggplotly(p)  

#reward
p<-ggplot(groupcomparison.dt[Motivation=="Reward"],aes(x=alpha_RiskyNoMeth-alpha_RiskyMeth))+geom_histogram(bins=30)+
  labs(title="Difference in posterior learning rate, NoMeth-Meth (Reward)")+
  geom_hdi(size=4, lineend = "round",color="blue",alpha=0.5,credible_mass=0.95)
plotly::ggplotly(p)
#punishment
p<-ggplot(groupcomparison.dt[Motivation=="Punishment"],aes(x=alpha_RiskyNoMeth-alpha_RiskyMeth))+geom_histogram(bins=30)+
  labs(title="Difference in posterior learning rate, NoMeth-Meth (Punishment)")+
  geom_hdi(size=4, lineend = "round",color="blue",alpha=0.5,credible_mass=0.95)
plotly::ggplotly(p)


# - Safe No Meth <> Risky No Meth, mu, {reward, punishment}

#overall
source("../visualization/geom_hdi.R")
p<-ggplot(groupcomparison.dt,aes(x=alpha_SafeNoMeth-alpha_RiskyNoMeth))+geom_histogram(bins=30)+
  labs(title="Difference in posterior learning rate, Safe-Risky NoMeth")+
  geom_hdi(size=4, lineend = "round",color="blue",alpha=0.5,credible_mass=0.95)
plotly::ggplotly(p)  

p<-ggplot(groupcomparison.dt,aes(x=alpha_RiskyNoMeth-alpha_RiskyMeth,fill=Motivation,color=Motivation))+geom_histogram(bins=30)+
  labs(title="Difference in posterior learning rate, Safe-Risky NoMeth")+
  geom_hdi(size=4, lineend = "round",alpha=0.9,credible_mass=0.95)
plotly::ggplotly(p)

#reward
p<-ggplot(groupcomparison.dt[Motivation=="Reward"],aes(x=alpha_RiskyNoMeth-alpha_RiskyMeth))+geom_histogram(bins=30)+
  labs(title="Difference in posterior learning rate, Safe-Risky NoMeth")+
  geom_hdi(size=4, lineend = "round",color="blue",alpha=0.5,credible_mass=0.95)
plotly::ggplotly(p)
#punishment
p<-ggplot(groupcomparison.dt[Motivation=="Punishment"],aes(x=alpha_RiskyNoMeth-alpha_RiskyMeth))+geom_histogram(bins=30)+
  labs(title="Difference in posterior learning rate, Safe-Risky NoMeth")+
  geom_hdi(size=4, lineend = "round",color="blue",alpha=0.5,credible_mass=0.95)
plotly::ggplotly(p)



# - 

```