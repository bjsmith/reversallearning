---
title: 'Final Report: Single level analysis'
output:
  pdf_document:
    keep_tex: yes
  html_document: 
    keep_md: yes
    self_contained: no
---

```{r setup, include=FALSE}


knitr::opts_chunk$set(echo = TRUE, tidy=TRUE)
knitr::opts_knit$set(root.dir="../")

```

```{r setup2, include=FALSE}

source("../util/apply_local_settings.R")
apply_local_settings()
knitr::opts_chunk$set(cache.path = paste0(localsettings$data.dir, "knitrcache"), warning = FALSE)
library(data.table)
library(ggplot2)
source("stanlba/lba_rl_joint_setup.R")
source("lba_rl_evaluate_functions.R")

```

I ran several single-level joint analyses. These ran quickly without the limitations I experienced running multi-level analyses for joint models.

##Limitations of joint models.

Both the Differential Evolution joint models and the stan NUTS models could not proceed. The Differential Evolution model was able to produce estimates using a two-level model. However, it was unstable in producing estimates at three levels.

% Remember? this was when I ran some massive, 10,000 iteration models; they seemd to run but they were messy and Brandon said the chains were unacceptably all over the place, which they kind of were.


### Experimenting with initialization

The three level NUTS model would initialize, run, and produce an estimate for small groups of as many as 15 subjects but could not properly intialize for larger groups. Because initializing seemed to be the main problem, I focused a lot on attempting to resolve initialization problems. The rstan MCMC estimator works by starting chains off at a particular point within the prior distribution, and calculates Jacobian transforms away from that point repeatedly, picking more likely proposals with a randomly determined probability. THus, it is important that the estimator starts at a point in the probability space that it can iterate from. If the point is too far at one extreme of the distribution, then the model may fail to properly intialize.

I thus experimented with several initialization methods, including randomization and bootstrap methods. For more information on these, see hierarchical\_noncentered\_compare\_starting\_values.

# Single-level models

With single-level models, it was easier to quickly run models and also to implement joint models. In single-level reversal learning models, I experimented with:

 - Different behavioral models, including a discrimination model, an OPAL-inspired model, a simple reinforcement learning model, and a LBA RL model.
 - centered and non-centered parameterizations
 - Variation of priors
 - Variation of covariance matrix estimation method
 
NEED TO EXPLAIN EACH OF THESE! 


## Single-level behavioral LBA RL model

Overall, I found that the behavioral LBA RL model was the best choice. I used a non-centered parameterization, and kept to priors I had been previously using, with a few exceptions. 

### Model

The model implemented Annis, Palmeri, Miller, as discussed elsewhere.

### Prior distributions

In order to facilitate ease of future development of the model into a hierarchical model, at all times I attempted to generate priors using conjugate distributions. Thus, parameter priors were all initially estimated on a normal distribution before being transformed into a more appropriate distribution. The $\alpha$ learning parameter is on a scale from 0 to 1, where zero represents no learning, and 1 represents a perfectly learned stimulus-response pair on each trial. An inverse logit transform converts $\alpha$ from the normal distribution into a logit distribution. At first, I had erroneously used large sigma values on the normal distribution with an aim of producing a non-informative prior. However, sigma values of greater than about 1.7 actually manipulate the shape of the alpha distribution to produce higher probabilities near the 0 and 1 point and lower probabilities at the midpoint. Thus, I settled on a prior for $\alpha$ of 1.5.

$k$ and $\tau$ were both estimated by starting with normal priors and were transformed to an exponential distribution with the inverse natural log function $e^x$. 

(we have the runs for this; consider adding them in!)

I followed recommended stan best practice ( need to add reference to this; I have done some writing on this already!) to use the following process for creating a covariance matrix:

 - Generate a lower-half Omega matrix using an LKJ cholesky correlation matrix function with an $\eta$ parameter of 1.
 - Generate a lower-half sigma matrix from a cauchy distribution with a $\mu=0, \sigma=1$, equal to the number of linked parameters.
 - Get the product of the diagonal matrix formed from the lower-half sigma value and the lower-half Omega matrix.
 - Generate a Sigma matrix using the product of the lower-half sigma matrix and its transpose.
 - Normalize each linked parameter by subtracting its mean and dividing the parameter by its standard deviation.
 - Estimate the log probability of the normalized parameter matrix given the Sigma matrix.
 
The normalized sigma matrix is effectively a correlation matrix.
 
I sampled from the prior distribution of the matrix derived from this method and confirmed that although not completely uninformative, with some bias toward zero covariance, priors were loose and enabled wide ranges of proposal covariance values to be considered.
 
### Regions
 
I tried analyzing freesurfer-derived regions as well as FSL-derived regions. There were adequately large correlations between freesurfer subcortical regions and FSL-derived regions to suggest they were capturing roughly the same regions. Correspondence between cortical regions could not be determined because each method--Freesurfer and FSL--subdivide the cortex using very different regional boundaries.

## Results

For the first model, I ran the model examining potential links with 25 selected freesurfer regions identified as part of the decision-making network.

```{r Resultsv11f_data, echo=FALSE,fig.height=6}
lba_rl_version<-"joint_20180917_1"
load("/expdata/bensmith/joint-modeling/data/msm/reversallearning/lba_rl/joint_20180917_1/lba_rl_single_exp_joint_v11s_rsdt.RData")
runs_missing_data<-runs_missing(rawdata,rsdt,FALSE)
```

`r runs_missing_data$runs_missing` runs did not complete. Runs fail to complete for several reasons, including timing out after 2 hours of failing to converge, or because $\hat{R}$ values are insufficiently close to 1, indicating poor convergence. There were `r runs_missing_data$subs_missing` subjects missing from the analysis in the analysis altogether.

```{r Resultsv11f_table_byRPE, echo=FALSE,fig.height=6}
results_raw<-data.table(read.csv("/expdata/bensmith/joint-modeling/data/msm/reversallearning/lba_rl/joint_20180917_1/lba_rl_single_exp_joint_v11s_revised.csv"))


results.rpe.present<-results_raw[Theta=="RPE",] %>% .[order(AdjustedPVals),]
results.rpe.present$Region<-factor(results.rpe.present$Region,levels=results.rpe.present[.N+1-order(AdjustedPVals),Region])

ggcorgraph<-list(geom_point(size=1),
                 geom_vline(xintercept=seq(1,40,2),color="#dddddd",linetype="dashed"),
                 geom_vline(xintercept=seq(2,40,2),color="#dddddd",linetype="dotted"),
                 geom_hline(yintercept = 0), 
                 geom_errorbar(aes(ymax=conf.int2,ymin=conf.int1)),
                 coord_flip(),
                 theme(legend.position = c(0.9,0.2)))

ggplot(results.rpe.present,
       aes(x=Region,y=estimate.mean.of.x,color=AdjustedPVals<0.05))+
  ggcorgraph+
  labs(title="Estimated mean correlation\nof regions with reward prediction error\n across subjects",
       y="Estimated mean correlation",
       color="p < 0.05",
       caption="Color-codes indicate a significant difference from zero at the FDR-corrected 0.05 level")
```

The Putamen correlated most strongly with the Reward Prediction Error. The Accumbens, cingulate gyrus (both ACC and PCG), and caudate correlated, though less strongly.


```{r Resultsv11e_table_byEV, echo=FALSE,asis=TRUE,fig.height=5}

results.ev.present<-results_raw[Theta=="EV",] %>% .[order(AdjustedPVals),]
results.ev.present$Region<-factor(results.ev.present$Region,levels=results.ev.present[.N+1-order(AdjustedPVals),Region])

ggplot(results.ev.present,
       aes(x=Region,y=estimate.mean.of.x,color=AdjustedPVals<0.05))+
  ggcorgraph+
  labs(title="Estimated mean correlation\nof regions with Expected Value\n across subjects",
       y="Estimated mean correlation",
       color="p < 0.05",
       caption="Color-codes indicate a significant difference from zero at the FDR-corrected 0.05 level")
  

```

Expected value not strongly linked to anything, but there was a weak, significant relationship with the frontal superior sulcus and putamen. There was a negative relationship betewen EV and regions right across theleft and right insula (i.e., low expected values were associated with higher insula activity).

Suborbital prefrontal regions did not show the expected effects. The suborbital sulcus was linked _negatively_ to expected value, and several other relevant areas - in particular, the rectus gyrus, did not significantly correlate with the modeled expected value signal.

ADD: count of subjects included in this analysis.

## Targeted analyses
```{r JustDeltaParamsData}

load("/expdata/bensmith/joint-modeling/data/msm/reversallearning/lba_rl/joint_20180917_1/lba_rl_single_exp_joint_v11t_rsdt.RData")
runs_missing_data<-runs_missing(rawdata,rsdt,FALSE)
```


I re-ran the single-level model to test whether we could get a stronger covariance result by focusing on a few regions. Using the LJK estimator, the estimates do covary between cells across samples and so it might be important to only include the few values we really think will show a correlation. In this analysis, `r runs_missing_data$runs_missing` runs were missing and `r runs_missing_data$subs_missing` subjects had no runs at all. The missing runs are a concern, and I believe that these missing runs could be driving the problems I have been having getting three-level models running.

## Correlation size and number of parameters

These tiny correlation estimates don't seem to be related to the number of parameters. When I run a model with just a Delta parameters, the sizes are no larger.

```{r JustDeltaParams1, echo=FALSE}
#region_correlation_strength_4_deltas<-data.table(read.csv("/expdata/bensmith/joint-modeling/data/msm/reversallearning/lba_rl/joint_20180709_1/lba_rl_single_exp_joint_v11g_provisional_raw.csv"))

region_correlation_strength_4_deltas<-data.table(read.csv("/expdata/bensmith/joint-modeling/data/msm/reversallearning/lba_rl/joint_20180917_1/lba_rl_single_exp_joint_v11t_revised.csv"))


results.present<-region_correlation_strength_4_deltas %>% .[order(AdjustedPVals),]
#results.present$Region<-factor(results.present$Region,levels=results.present[.N+1-order(AdjustedPVals),Region])
results.present$AdjustedPValsSignificant<-results.present$AdjustedPVals<0.05
results.present$AdjustedPValsSignificant<-factor(as.character(results.present$AdjustedPValsSignificant),levels=c("FALSE","TRUE"))
ggplot(results.present,
       aes(x=Region,y=estimate.mean.of.x,color=AdjustedPValsSignificant))+
  ggcorgraph+facet_grid(~Theta)+
  labs(title="Estimated mean correlation\nof regions with Expected Value and Reward Prediction Error \n across subjects",
       y="Estimated mean correlation",
       color="p < 0.05",
       caption="Color-codes indicate a significant difference from zero at the FDR-corrected 0.05 level")+theme(legend.position=c(0.5,0.2))

```

The results are significant for reward prediction error, but not for expected value, consistent with the previous test. Both regions, the suborbital sulcus and accumbens, are positively associated with reward prediction error, while neither are significantly related to the Accumbens. This is a partial confirmation of the hypothesis that the Accumbens would be associated with reward prediction error while the suborbital sulcus would be associated with expected value. 


