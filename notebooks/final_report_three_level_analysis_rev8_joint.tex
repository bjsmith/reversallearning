\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Exploring Revision 8 Joint model rev 4 MCMC},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Exploring Revision 8 Joint model rev 4 MCMC}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  

\begin{document}
\maketitle

I ran Revision 8 using \texttt{rstan} MCMC, running separately on each
of the three risk groups.

\subsection{Method}\label{method}

The model was the same as in the previous section, except that a
correlation matrix was added in which the correlation between four brain
regions was estimated.

To add the correlation matrix, several changes needed to be made to the
stan code. First, in the parameters block, we declare two additional
parameters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters\{}
  \NormalTok{...}
  \NormalTok{cholesky_factor_corr[TD_N] L_Omega;}
  \NormalTok{vector<lower=}\DecValTok{0}\NormalTok{>[TD_N] L_sigma;}
\NormalTok{\}}
\NormalTok{transformed parameters\{}
  \NormalTok{...}
  \NormalTok{matrix[TD_N, TD_N] L_Sigma = diag_pre_multiply(L_sigma, L_Omega);}
  \NormalTok{...}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

In the transformed parameters block, we also define the \(\Sigma\)
matrix itself; the name is preceded with an `L', indicating the matrix
only stores the lower half of the matrix.

Next, the model block contains the command to specify the distribution
from which priors for \textbar{}Sigma\textbar{} would be drawn are
specified. The model block also contains the key commands to estimate
the correlation between the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\{}
  \NormalTok{...}
  
  \KeywordTok{for} \NormalTok{(r in }\DecValTok{1}\NormalTok{:NUM_RUNS)\{}
    \NormalTok{...}
    \NormalTok{theta_delta[:,THETA_rpe]=logit(run_pred_err_c2/}\DecValTok{4}\FloatTok{+0.5}\NormalTok{);}
    \NormalTok{theta_delta[:,THETA_ev]=logit(trial_expected_val/}\DecValTok{2}\FloatTok{+0.5}\NormalTok{);}
    
    \CommentTok{//transfer the deltas into the theta-delta matrix.}
    \KeywordTok{for} \NormalTok{(d_i in }\DecValTok{1}\NormalTok{:DELTA_N)\{}
      \NormalTok{theta_delta[:,THETA_N+d_i]=neural_data[run_first_trial_id:run_last_trial_id, d_i];}
    \NormalTok{\}}
  
    \KeywordTok{for} \NormalTok{(tdi in }\DecValTok{1}\NormalTok{:TD_N)\{}
      \NormalTok{td_mean[tdi] = mean(theta_delta[:,tdi]);  }\CommentTok{//separately calculate mean for each ThetaDelta var}
      \NormalTok{td_sd[tdi] = sd(theta_delta[:,tdi]);      }\CommentTok{//separately calculate SD for each ThetaDelta var}
    \NormalTok{\}}
  
    \KeywordTok{for} \NormalTok{(i in }\DecValTok{1}\NormalTok{:run_N_trials[r])\{}
      \NormalTok{td_var[i,:] = (to_vector(theta_delta[i,:]) - td_mean) ./ td_sd;}
    \NormalTok{\}}
  
    \NormalTok{L_Omega ~ lkj_corr_cholesky(}\DecValTok{1}\NormalTok{);}
    \NormalTok{L_sigma ~ cauchy(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{); }\CommentTok{//these yield standard deviations of each individual value.}
    \NormalTok{td_var ~ multi_normal_cholesky(zeros,L_Sigma);}
    
  \NormalTok{\}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The second and third lines simply copy the expected value and reward
prediction error into the \textbar{}theta\_delta\textbar{} matrix of
behavioral variables (`thetas') and neural data (`deltas'), transforming
them into a space with infinite support (i.e., in the range of negative
to positive infinity). The following \textbar{}for\textbar{} loop copies
the neural data into that same matrix. Then, a set of commands normalize
each column in the matrix, so that each variable is normally distributed
with a mean of zero and standard deviation of 1. This makes
interpretation simpler, because we can simply calculate a correlation
matrix rather than a covariance matrix, and these will be equivalent.
The final set of three command actually estimates values; L\_Omega
estimates correlation, while L\_sigma estimates covariance. (I should
note the covariance estimation here is redundant because of the
transformation of values to standard space and if I was to run this
again, I would remove this redundancy and focus on modeling correlation.
However, I have run some limited tests for a subset of subjects and
found that a correlation-based estimate without this redundancy produces
similar results.)

I am not aware of any prior implementations of Turner's joint modeling
paradigm in stan; the closest analogue was implemented in jags, and was
not hierarchical. Thus, I tested the ability of this code to recover
synthetic data, and it was accurately able to recover synthetic
data.\footnote{need to insert; both the completely synthetic data and the partially synthetic tests I may have done, related to looking at the distribuions these commands produced.}

One novel element to this design is that the correlation matrix is
estimated using the same prior matrix independently for each run. This
may cause problems if there is a large variance in the actual
correlation matrices existing across subjects, but if there are
similarities then the model should allow us to recover a correlation
matrix reflecting correlations between each of the theta and delta
variables.

\begin{verbatim}
## [1] "initializing..."
\end{verbatim}

\subsection{Results}\label{results}

\subsubsection{Diagnostics: Reliablity, representativeness, and
accuracy}\label{diagnostics-reliablity-representativeness-and-accuracy}

As in the previous section, I assess representativeness for an MCMC
algorithm by looking at the Gelman-Rubin statistic (Gelman \& Rubin,
1992) chain convergence statistic, \(\widehat{r}\).

\begin{longtable}[]{@{}lrrrrr@{}}
\caption{group= 1 double\_update\_jointrev4 1 MCMC vars=
967}\tabularnewline
\toprule
& mean & se\_mean & sd & n\_eff & Rhat\tabularnewline
\midrule
\endfirsthead
\toprule
& mean & se\_mean & sd & n\_eff & Rhat\tabularnewline
\midrule
\endhead
group\_mu\_alpha & 0.76 & 0.11 & 0.26 & 6 & 148.40\tabularnewline
group\_mu\_beta & 3.46 & 1.71 & 4.18 & 6 & 145.12\tabularnewline
group\_rew\_mu\_alpha & 0.80 & 0.09 & 0.22 & 6 & 125.53\tabularnewline
group\_pun\_mu\_alpha & 0.72 & 0.12 & 0.30 & 6 & 202.09\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lrrrrr@{}}
\caption{group= 1 double\_update\_jointrev4 1 NA vars=
967}\tabularnewline
\toprule
& mean & se\_mean & sd & n\_eff & Rhat\tabularnewline
\midrule
\endfirsthead
\toprule
& mean & se\_mean & sd & n\_eff & Rhat\tabularnewline
\midrule
\endhead
group\_mu\_alpha & 0.76 & 0.11 & 0.26 & 6 & 148.40\tabularnewline
group\_mu\_beta & 3.46 & 1.71 & 4.18 & 6 & 145.12\tabularnewline
group\_rew\_mu\_alpha & 0.80 & 0.09 & 0.22 & 6 & 125.53\tabularnewline
group\_pun\_mu\_alpha & 0.72 & 0.12 & 0.30 & 6 & 202.09\tabularnewline
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lrrrrr@{}}
\caption{group= 1 double\_update\_jointrev4 1 NA vars=
967}\tabularnewline
\toprule
& mean & se\_mean & sd & n\_eff & Rhat\tabularnewline
\midrule
\endfirsthead
\toprule
& mean & se\_mean & sd & n\_eff & Rhat\tabularnewline
\midrule
\endhead
group\_mu\_alpha & 0.76 & 0.11 & 0.26 & 6 & 148.40\tabularnewline
group\_mu\_beta & 3.46 & 1.71 & 4.18 & 6 & 145.12\tabularnewline
group\_rew\_mu\_alpha & 0.80 & 0.09 & 0.22 & 6 & 125.53\tabularnewline
group\_pun\_mu\_alpha & 0.72 & 0.12 & 0.30 & 6 & 202.09\tabularnewline
\bottomrule
\end{longtable}

As can be seen in the tables, \(\widehat{r}\) values are substantially
outside the acceptable range of \(\widehat{r} < 1.05\). The
\texttt{n\_eff} column offers a hint why these \(\widehat{r}\) values
are so poor: for some reason, this model was not able produce acceptable
samples. Most likely, the prior distributions used do not closely enough
represent the space inhabited by the data, and so the sample space
cannot be explored effectively.

\subsection{Discussion}\label{discussion}

There may be several ways to improve on this implementation. First, it
may be that another approach to modeling across the runs is needed. In
this implementation, I simply modeled the theta-delta correlation using
the same matrix for all runs. In practice, this will vary between runs.
It may be more suitable to generate the matrix using a hyper-parameter
and generate a posterior sample from that run.

Alternatively, theta-deltas across all runs could be inserted into a
single large matrix and the correlation of each could be measured in
that matrix. This might complicate the interpretation, because it would
include both within- and between-run variance. The current design
strictly measures the within-run correlation of theta and delta values
in a way that is generalizable to other runs. However, we could overcome
some of the problem by normalizing the data from each run separately.

I have seen from the single level models (REF) that it is unlikely there
are strong correlations between the neural and behavioral parameters.
This should \emph{not} be a problem for the joint model presented here,
because the prior for the correlation matrix allows for zero
correlations.


\end{document}
