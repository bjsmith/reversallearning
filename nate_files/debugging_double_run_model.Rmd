---
title: "Debugging double run model.Rmd"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("fitGroupsV3Onegroup.R")
```

## Grabbing the data from the fuller model

```{r getsomesampledata}

fit.RiskyNoMeth<-lookupOrRunFit(
  run=c(1,2),groups_to_fit=2, model_to_use="double_update_rp_repeated_runs",includeSubjGroup = FALSE,
  rp=c(REVERSAL_LEARNING_REWARD,REVERSAL_LEARNING_PUNISHMENT),
  model_rp_separately=TRUE,model_runs_separately = TRUE, include_pain=FALSE)

fit.RiskyNoMeth.ex<-rstan::extract(fit.RiskyNoMeth$fit)
rm(fit.RiskyNoMeth)#these are large files; let's not keep them in memory where unnecessary.

fit.RiskyMeth<-lookupOrRunFit(run=c(1,2),groups_to_fit=3, 
                              model_to_use="double_update_rp_repeated_runs",includeSubjGroup = FALSE,
                              rp=c(REVERSAL_LEARNING_REWARD,REVERSAL_LEARNING_PUNISHMENT),
                              model_rp_separately=TRUE,model_runs_separately = TRUE, include_pain=FALSE)
fit.RiskyMeth.ex<-rstan::extract(fit.RiskyMeth$fit)
rm(fit.RiskyMeth)#these are large files; let's not keep them in memory where unnecessary.

```

OK, let's take alook at the data:
```{r learning_rate_change_runs, echo=TRUE}

#source("fitGroupsV3Onegroup.R")
 
dim(fit.RiskyNoMeth.ex$alpha_rew_pr_run_multiplier)
dim(fit.RiskyNoMeth.ex$alpha_rew_run_multiplier)
dim(fit.RiskyNoMeth.ex$mu_p_rm)
dim(fit.RiskyNoMeth.ex$mu_alpha_rew_run_multiplier)
dim(fit.RiskyNoMeth.ex$mu_alpha_rew)
dim(fit.RiskyNoMeth.ex$sigma_rm)

runMultiplierTable<-function(f,group_name){rbind(
  data.table(
  mean=f$mu_alpha_rew_run_multiplier[,1],
  variance=f$sigma_rm[,1,1,1],
  mode="reward",Group=group_name),
  data.table(
    mean=f$mu_alpha_pun_run_multiplier[,1],
    variance=f$sigma_rm[,2,1,1],
    mode="punishment",Group=group_name))
}

dim(fit.RiskyNoMeth.ex$mu_alpha_rew_run_multiplier)
alpha.runMultiplier.g2<-runMultiplierTable(fit.RiskyNoMeth.ex,"RiskyNoMeth")
alpha.runMultiplier.g3<-runMultiplierTable(fit.RiskyMeth.ex,"RiskyMeth")
alpha.runMultiplier<-rbind(alpha.runMultiplier.g2,alpha.runMultiplier.g3)
ggplot(alpha.runMultiplier,aes(x=mean,y=variance,colour=Group))+geom_point(alpha=0.5)+
  facet_grid(.~mode)+
  labs(title="Posterior group-level Run2/Run1\n learning rate ratio and variance for reward and punishment\n(Both Runs)")



```
Now let's take a look ate the beta data:

```{r learning_rate_change_runs_beta, echo=TRUE}

runMultiplierBetaTable<-function(f,group_name){rbind(
  data.table(
  mean=f$mu_beta_rew_run_multiplier[,1],
  variance=f$sigma_rm[,1,1,2],
  mode="reward",Group=group_name),
  data.table(
    mean=f$mu_beta_pun_run_multiplier[,1],
    variance=f$sigma_rm[,2,1,2],
    mode="punishment",Group=group_name))
}
beta.runMultiplier.g2<-runMultiplierBetaTable(fit.RiskyNoMeth.ex,"RiskyNoMeth")
beta.runMultiplier.g3<-runMultiplierBetaTable(fit.RiskyMeth.ex,"RiskyMeth")
beta.runMultiplier<-rbind(beta.runMultiplier.g2,beta.runMultiplier.g3)
ggplot(beta.runMultiplier,aes(x=mean,y=variance,colour=Group))+geom_point(alpha=0.5)+
  facet_grid(.~mode)+
  labs(title="Posterior group-level Run2/Run1 inverse temperature ratio and variance for reward and punishment\n(Both Runs)")

```

These don't seem particularly informative and I suspect there is an error with the way they ahve been calculated. In particular, ratios for "alpha" (learning rate) and "beta" (inverse temperature) seem identical. There's got ot be something wrong here. Let's take a look at a small dataset.

```{r ReducedDataset}

fit.RiskyNoMeth.fast<-lookupOrRunFit(
  run=c(1,2),groups_to_fit=2, model_to_use="double_update_rp_repeated_runs",includeSubjGroup = FALSE,
  rp=c(REVERSAL_LEARNING_REWARD,REVERSAL_LEARNING_PUNISHMENT),
  model_rp_separately=TRUE,model_runs_separately = TRUE, include_pain=FALSE)

fit.RiskyNoMeth.fast.ex<-rstan::extract(fit.RiskyNoMeth.fast$fit)
rm(fit.RiskyNoMeth.fast)#these are large files; let's not keep them in memory where unnecessary.

dim(fit.RiskyNoMeth.fast.ex$mu_alpha_rew)
dim(fit.RiskyNoMeth.fast.ex$mu_alpha_pun)
dim(fit.RiskyNoMeth.fast.ex$mu_beta_rew)
dim(fit.RiskyNoMeth.fast.ex$mu_beta_pun)


runMultiplierTable<-function(f,group_name){rbind(
  data.table(
    mean=f$mu_alpha_rew[,1],
    variance=f$sigma_rm[,1,1,1],
    mode="reward",Group=group_name
    ),
  data.table(
    mean=f$mu_alpha_pun_run_multiplier[,1],
    variance=f$sigma_rm[,2,1,1],
    mode="punishment",Group=group_name))
}

hist(fit.RiskyNoMeth.fast.ex$mu_alpha_rew[,1])
hist(fit.RiskyNoMeth.fast.ex$mu_alpha_rew[,2])
hist(fit.RiskyNoMeth.fast.ex$mu_alpha_pun[,1])
hist(fit.RiskyNoMeth.fast.ex$mu_alpha_pun[,2])

hist(fit.RiskyNoMeth.fast.ex$mu_beta_rew[,1])
hist(fit.RiskyNoMeth.fast.ex$mu_beta_rew[,2])
hist(fit.RiskyNoMeth.fast.ex$mu_beta_pun[,1])
hist(fit.RiskyNoMeth.fast.ex$mu_beta_pun[,2])

t.test(
  fit.RiskyNoMeth.fast.ex$mu_alpha_rew[,1],
  fit.RiskyNoMeth.fast.ex$mu_alpha_rew[,2]
  )
#learning rate seems to improve from round 1 to round 2

t.test(
  fit.RiskyNoMeth.fast.ex$mu_alpha_pun[,1],
  fit.RiskyNoMeth.fast.ex$mu_alpha_pun[,2]
  )
#for reward and punishment

t.test(
  fit.RiskyNoMeth.fast.ex$mu_beta_rew[,1],
  fit.RiskyNoMeth.fast.ex$mu_beta_rew[,2]
  )
t.test(
  fit.RiskyNoMeth.fast.ex$mu_beta_pun[,1],
  fit.RiskyNoMeth.fast.ex$mu_beta_pun[,2]
  )


runMultiplierTable(fit.RiskyNoMeth.fast.ex)

```

These look good. Let's use this!