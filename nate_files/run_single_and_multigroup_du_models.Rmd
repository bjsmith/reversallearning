---
title: "run single and multigroup double update models"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(rstan)
library(ggplot2)
```

# double update model, single groups


First, we want to try fitting the risky meth group. After that we can try fitting the risky non-meth group.

So far these are constrained to one run only each. The extra run would add extra complications. We need to add one additional complication at a time!

```{r double_update_risky_nonmeth_group, echo=TRUE}

source("fitGroupsV3OnegroupRun1.R")

fit<-lookupOrRunFit(run=1,2,model_to_use="double_update")

#try to get some info about the fit.
#mu_p[1] is estimated mean alpha [learning rate] across subjects
#mu_p[2] is estimated mean beta [inverse temperature]
#sigma[1] is estimated variance alpha
#sigma[2] is estimated variance beta
#Accessing the contents of a stanfit object
#https://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.html

fit.extracted<-rstan::extract(fit$fit)
#these parameters are not actually like the mean learning rate; 
#more like the 'intercept' in a linear model of the learning rate
#across subjects.

learning.rate.estimate<-data.table(mean=fit.extracted$mu_alpha,
                                   variance=fit.extracted$sigma[,1])
inverse.temperature.estimate<-data.table(mean=fit.extracted$mu_beta,
                                   variance=fit.extracted$sigma[,2])
ggplot(learning.rate.estimate,aes(x=mean,y=variance))+geom_point()
ggplot(inverse.temperature.estimate,aes(x=mean,y=variance))+geom_point()
```

Apparently we have negative learning rates and inverse temperatures here.

What about run 1, meth group?
```{r double_update_risky_meth_group, echo=TRUE}

source("fitGroupsV3OnegroupRun1.R")

fit<-lookupOrRunFit(run=1,3,model_to_use="double_update")

#try to get some info about the fit.
#mu_p[1] is estimated mean alpha [learning rate] across subjects
#mu_p[2] is estimated mean beta [inverse temperature]
#sigma[1] is estimated variance alpha
#sigma[2] is estimated variance beta
#Accessing the contents of a stanfit object
#https://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.html

fit.extracted<-rstan::extract(fit$fit)

learning.rate.estimate<-data.table(mean=fit.extracted$mu_alpha,
                                   variance=fit.extracted$sigma[,1])
inverse.temperature.estimate<-data.table(mean=fit.extracted$mu_beta,
                                   variance=fit.extracted$sigma[,2])
ggplot(learning.rate.estimate,aes(x=mean,y=variance))+geom_point()
ggplot(inverse.temperature.estimate,aes(x=mean,y=variance))+geom_point()

```


OK. now, we want to look into doing more than one group. Or more than one run.

Let's start with expanding to multiple groups, since I have already written the code for that!

```{r double_update_risky_meth_group, echo=TRUE}

source("fitGroupsV3OnegroupRun1.R")

fit<-lookupOrRunFit(run=1,c(2,3), model_to_use="prl_ben_v3_group",includeSubjGroup = TRUE)

#try to get some info about the fit.
#mu_p[1] is estimated mean alpha [learning rate] across subjects
#mu_p[2] is estimated mean beta [inverse temperature]
#sigma[1] is estimated variance alpha
#sigma[2] is estimated variance beta
#Accessing the contents of a stanfit object
#https://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.html

fit.extracted<-rstan::extract(fit$fit)

learning.rate.estimate<-data.table(mean=fit.extracted$mu_alpha,
                                   variance=fit.extracted$sigma[,1])
inverse.temperature.estimate<-data.table(mean=fit.extracted$mu_beta,
                                   variance=fit.extracted$sigma[,2])
ggplot(learning.rate.estimate,aes(x=mean,y=variance))+geom_point()
ggplot(inverse.temperature.estimate,aes(x=mean,y=variance))+geom_point()

```

OK, so we are getting convergence problems here. Nathan expects that when we try to estimate two independent groups at once.
So perhaps we should do each group separately? If we're doing that, then the next logical step is to take the double update model and try to add separate reward and punishment learning rates :-)

```{r double_update_risky_nometh_single_group_reward_and_punishment, echo=TRUE}

source("fitGroupsV3OnegroupRun1.R")

#risky nometh
fit<-lookupOrRunFit(run=1,groups_to_fit=2, model_to_use="double_update_rp",includeSubjGroup = FALSE,
                    rp=c(1,2),model_rp_separately=TRUE,include_pain=FALSE)


#risky meth....

#try to get some info about the fit.
#mu_p[1] is estimated mean alpha [learning rate] across subjects
#mu_p[2] is estimated mean beta [inverse temperature]
#sigma[1] is estimated variance alpha
#sigma[2] is estimated variance beta
#Accessing the contents of a stanfit object
#https://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.html

fit.RiskyNoMeth.extracted<-rstan::extract(fit$fit)

learning.rate.estimate.g2<-rbind(data.table(mean=fit.extracted$mu_alpha_rew,
                                   variance=fit.extracted$sigma[,1,1],mode="reward",Group="RiskyNoMeth"),
                              data.table(mean=fit.extracted$mu_alpha_pun,
                                   variance=fit.extracted$sigma[,2,1],mode="punishment",Group="RiskyNoMeth"))

inverse.temperature.estimate.g2<-rbind(data.table(mean=fit.extracted$mu_beta_rew,
                                   variance=fit.extracted$sigma[,1,2],mode="reward",Group="RiskyNoMeth"),
                              data.table(mean=fit.extracted$mu_beta_pun,
                                   variance=fit.extracted$sigma[,2,2],mode="punishment",Group="RiskyNoMeth"))

fit.RiskyMeth<-lookupOrRunFit(run=1,groups_to_fit=3, model_to_use="double_update_rp",includeSubjGroup = FALSE,
                    rp=c(1,2),model_rp_separately=TRUE,include_pain=FALSE)

fit.RiskyMeth.extracted<-rstan::extract(fit.RiskyMeth$fit)

learning.rate.estimate<-rbind(learning.rate.estimate.g2,data.table(mean=fit.RiskyMeth.extracted$mu_alpha_rew,
                                   variance=fit.RiskyMeth.extracted$sigma[,1,1],mode="reward",Group="RiskyMeth"),
                              data.table(mean=fit.RiskyMeth.extracted$mu_alpha_pun,
                                   variance=fit.RiskyMeth.extracted$sigma[,2,1],mode="punishment",Group="RiskyMeth"))

inverse.temperature.estimate<-rbind(inverse.temperature.estimate.g2,data.table(mean=fit.RiskyMeth.extracted$mu_beta_rew,
                                   variance=fit.RiskyMeth.extracted$sigma[,1,2],mode="reward",Group="RiskyMeth"),
                              data.table(mean=fit.RiskyMeth.extracted$mu_beta_pun,
                                   variance=fit.RiskyMeth.extracted$sigma[,2,2],mode="punishment",Group="RiskyMeth"))

ggplot(learning.rate.estimate,aes(x=mean,y=variance,colour=Group))+geom_point()+
  facet_grid(.~mode)+
  labs(title="Estimated group-level learning rate and variance for reward and punishment\n(Run 1)")

ggplot(inverse.temperature.estimate,aes(x=mean,y=variance,colour=Group))+geom_point()+
  facet_grid(.~mode)+
  labs(title="Estimated group-level inverse temperature and variance for reward and punishment\n(Run 1")

#what would a confidence interval of these look like? We have 1000 samples, so we should calculate, for 1000 samples, what the 95% confidence interval is for the mean and variance parameters.

#it's arbitrary, but we can just combine for each sample in group 1 with each sample in group 2
#that would give us the confidence interval of the mean and variance parameter. I then need to think carefully about how these should be represented as confidence intervals of the population.
#so first: for mu_alpha_rew
group1.mu<-fit.RiskyNoMeth.extracted$mu_alpha_rew
group2.mu<-fit.RiskyMeth.extracted$mu_alpha_rew
mu.diff<-group2.mu-group1.mu#so in all possible samples the difference in groups in mean alpha parameters exists
#but what about the variance parameter? 
#We know that these two particular groups always differ, but does that really tell us that across the population, this is the same?
#What we have is samples representing the probability space of a normal distribution.
#We need to iterate through each sample and find out, for each sample, do the distributions differ 95% of the time?
#I might need to re-read the chapter from Kruschke to get a good intuition here because I don't know whether:
# - we can just take a difference in the group mean parameter to mean a population difference definitely exists...
# - or whether we need to do random sampling from the 1000 sample distributions or
# - whether we can use an analytical approach on the 1000 sample distributions
```

#Multiple runs

We have expanded the stan model to handle both reward and punishment parameters. But currently the model only handles one run.
We can expand the model to calculate for an arbitrary number of runs. We add a "run multiplier" parameter for each subject-level parameter and run.
The run multiplier is set to 1 for the first run, and can vary freely for subsequent runs.
There is an independent run multiplier parameter for each subject, for reward and punishment trials, for each parameter (alpha and beta) and each run after the first run.
At the group level, the run multiplier (except for the first) for each subject comes from a population of run multipliers.
There is a mean and variance of the population of run multipliers, just as there is for the population of alphas and betas.
Although our data only has up to two runs per subject, this does handle an arbitrary number of runs. However, there may be problems dealing with subjects who have fewer than the normal number of runs, and we might have to record the number of runs per subject in order to take that into acccount.

The model is run as follows:

```{r double_update_reward_and_punishment_multiple runs, echo=TRUE}

source("fitGroupsV3Onegroup.R")

learningRateTable<-function(f,group_name){rbind(
  data.table(
  mean=f$mu_alpha_rew,
  variance=f$sigma[,1,1],mode="reward",Group=group_name),
  data.table(
    mean=f$mu_alpha_pun,
    variance=f$sigma[,2,1],
    mode="punishment",Group=group_name))
}


inverseTemperatureTable<-function(f,group_name){rbind(
  data.table(
  mean=f$mu_beta_rew,
  variance=f$sigma[,1,2],
  mode="reward",Group=group_name),
  data.table(
    mean=f$mu_beta_pun,
    variance=f$sigma[,2,2],
    mode="punishment",Group=group_name))
}


fit.RiskyNoMeth<-lookupOrRunFit(
  run=c(1,2),groups_to_fit=2, model_to_use="double_update_rp_repeated_runs",includeSubjGroup = FALSE,
  rp=c(REVERSAL_LEARNING_REWARD,REVERSAL_LEARNING_PUNISHMENT),
  model_rp_separately=TRUE,model_runs_separately = TRUE, include_pain=FALSE)

# load("Fits/double_update_rp_repeated_runs_Risky_NoMeth_rewpun_separate_run12_model_distinct_runs_good_1.RData")
# goodfit<-fit_data
# load("Fits/double_update_rp_repeated_runs_Risky_NoMeth_rewpun_separate_run12_model_distinct_runs_bad_1.RData")
# badfit<-fit_data
fit.RiskyNoMeth.ex<-rstan::extract(fit.RiskyNoMeth$fit)
rm(fit.RiskyNoMeth)#these are large files; let's not keep them in memory where unnecessary.

learning.rate.estimate.g2<-learningRateTable(fit.RiskyNoMeth.ex,"RiskyNoMeth")

inverse.temperature.estimate.g2<-inverseTemperatureTable(fit.RiskyNoMeth.ex,"RiskyNoMeth")

fit.RiskyMeth<-lookupOrRunFit(run=c(1,2),groups_to_fit=3, 
                              model_to_use="double_update_rp_repeated_runs",includeSubjGroup = FALSE,
                              rp=c(REVERSAL_LEARNING_REWARD,REVERSAL_LEARNING_PUNISHMENT),
                              model_rp_separately=TRUE,model_runs_separately = TRUE, include_pain=FALSE)

fit.RiskyMeth.ex<-rstan::extract(fit.RiskyMeth$fit)

rm(fit.RiskyMeth)#these are large files; let's not keep them in memory where unnecessary.

learning.rate.estimate.g3<-learningRateTable(fit.RiskyMeth.ex,"RiskyMeth")
inverse.temperature.estimate.g3<-inverseTemperatureTable(fit.RiskyMeth.ex,"RiskyMeth")


learning.rate.estimate<-rbind(learning.rate.estimate.g2,learning.rate.estimate.g3)
inverse.temperature.estimate<-rbind(inverse.temperature.estimate.g2,inverse.temperature.estimate.g3)



ggplot(learning.rate.estimate,aes(x=mean,y=variance,colour=Group))+geom_point()+
  facet_grid(.~mode)+
  labs(title="Posterior group-level learning rate and variance for reward and punishment\n(Both Runs)")

ggplot(inverse.temperature.estimate,aes(x=mean,y=variance,colour=Group))+geom_point()+
  facet_grid(.~mode)+
  labs(title="Posterior group-level inverse temperature and variance for reward and punishment\n(Both Runs)")


```
We can present these in a slightly more principled way, too - it might be a bit weird to plot variance against mean as above. So let's plot variance as error bars.
```{r double_update_reward_and_punishment_multipleruns_errorbars, echo=TRUE}

lritTable<-function(f,group_name){rbind(
  data.table(
  alpha_mean=f$mu_alpha_rew,
  alpha_variance=f$sigma[,1,1],
  beta_mean=f$mu_beta_rew,
  beta_variance=f$sigma[,1,2],
  mode="reward",Group=group_name),
  data.table(
    alpha_mean=f$mu_alpha_pun,
    alpha_variance=f$sigma[,2,1],
    beta_mean=f$mu_beta_pun,
    beta_variance=f$sigma[,2,2],
    mode="punishment",Group=group_name))
}
lrit.estimate<-rbind(lritTable(fit.RiskyNoMeth.ex,"RiskyNoMeth"),lritTable(fit.RiskyMeth.ex,"RiskyMeth"))
colnames(lrit.estimate)
ggplot(lrit.estimate,aes(x=alpha_mean,y=beta_mean,colour=Group))+
  geom_errorbarh(aes(
    xmin=alpha_mean-alpha_variance, xmax=alpha_mean+alpha_variance,
    
    ), width=.1,alpha=0.01)+
  geom_errorbar(aes(
    ymin=beta_mean-beta_variance, ymax=beta_mean+beta_variance,
    
    ), width=.1,alpha=0.01)+
  geom_point()+
  facet_grid(.~mode)+
  labs(title="Posterior group-level learning rate and inverse temperature\nfor reward and punishment (Both Runs)")

```


So - we see pretty clear patterns here. Subjects appear to learn faster for Reward than Punishment, at least the RiskyMeth group, and faster for the RiskyNoMeth group than the RiskyMeth group.

However, variance is reasonably high, so although one group clearly has a lower average than the other, there is a lot of overlap between the groups.

Looking at the inverse temperature parameter, RiskyMeth subjects definitely have higher inverse temperatures. For both groups, inverse temperature is higher for reward than punishment. A higher inverse temperature indicates *more* consistency among responses - in other words, RiskyMeth subjects appear to be more *consistent* in their responses than RiskyNonMeth subjects.

## Speed
Can we speed this up at all? This takes a look at a model where reward and punishment have been vectorized. Hopefully, this will make things faster.

```{r Vectorized}
#first iteration.
fit.RiskyNoMeth.fast<-lookupOrRunFit(
  run=c(1,2),groups_to_fit=2, model_to_use="double_update_rpo_repeated_runs",includeSubjGroup = FALSE,
  rp=c(REVERSAL_LEARNING_REWARD,REVERSAL_LEARNING_PUNISHMENT),
  model_rp_separately=TRUE,model_runs_separately = TRUE, include_pain=FALSE,fastDebug = FALSE)
```


Let's take a look at:
 * Whether subjects change their learning rate or inverse temperature from one run to the next.
 * Calculate HDIs for these averages
 * Get posterior predictions per subject
 * Describe the odds of an individual subject in the first group having a higher or lower learning rate 
 than an individual subject in the second goup. This would probably involve modifying the model's generated quantities to generate a sample of subjects.
 
 
#Learning rate ratios 

For learning rate ratios, see debugging_double_run_model.Rmd

#HDIs
```{r calc_hdi, echo=TRUE}
library(hBayesDM)
HDIofMCMC(fit.RiskyMeth.ex$mu_alpha_pun_run_multiplier[,1])
HDIofMCMC(fit.RiskyNoMeth.ex$mu_alpha_pun_run_multiplier[,1])

HDIofMCMC(fit.RiskyNoMeth.extracted$mu_beta_rew)
HDIofMCMC(fit.RiskyNoMeth.extracted$mu_alpha_rew)
```