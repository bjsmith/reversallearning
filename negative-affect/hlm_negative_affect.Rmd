---
title: "Negative affect behavioral data"
output: html_notebook
---
Verifying pain signal

If the NPS similarity reliably measures pain, then we should see that the NPS score is higher in trials where subjects receive an electric shock compared to trials where they do not. Thus, sign that the NPS is measuring pain is to ensure that:

 1. In the punishment runs, NPS scores should be higher in those trials where subjects receive an electric shock than those where they did not, i.e., where they made an incorrect response or made no response compared to when they made a correct response.
 2. The difference between incorrect or non-responses and correct responses should be substantially stronger in the punishments than in the reward runs, where subjects were being rewarded for correct responses, but not punished for incorrect responses. If this is indeed the case, then we have evidence the NPS is measuring the brain response to physical pain. If there is no difference, then the NPS might be detecting non-physical pain, or a negative signal which is not physical pain.

After the NPS scores were obtained, I tested these two postulates with a series of linear models.

```{r, include=FALSE,cache=TRUE}
setwd("../")
source("negative-affect/negative_affect_trials_setup.R")
```

## NPS scores higher in trials where subjects receive an electric shock

The first set of models tests our ability distinguish between punished trials (incorrect or non-response) and non-punished trials (correct results) in the punishment runs.

We first build a base level model, for comparison, that attempts to predict the NPS signal on a given trial without reference to whether or not that trial was correct.


```{r}
m.0<-lmer(ValueScaled~
                     presentation_n_in_segment + 
                     (1+presentation_n_in_segment | subid/runid) + 
                     (1 | image),pain_data)
```

We represent subject as $S$, run as $S$, and image as $\iota$; then 

$V=\mathit{PRESENTATION}_{i} + S_{s(i)} + R_{r(s,i)} + \iota_{j(i)} + \epsilon$

```{r}
m.sronly<-lmer(ValueScaled~presentation_n_in_segment+ (1 | subid/runid),pain_data)
summary(m.sronly)
```


Then we can repeat the model, this time with the correct response included:

$V=\mathit{RESPONSE}_k*\mathit{PRESENTATION}_{i} + S_{s(i, k )} + R_{r(s,i , k )} + \iota_{j(i, k)} + \epsilon$

```{r}

m.withReponseCorrect<-lmer(ValueScaled~ResponseCorrect + presentation_n_in_segment + (1+ResponseCorrect | subid/runid),pain_data)
summary(m.withReponseCorrect)


```

Over all pain datasets, the model with correct response has much lower AIC and BIC values, incdicating correct response explains variance within the data.


```{r}
anova(m.sronly,m.withReponseCorrect)
```

This suggests that responses are predictive of the NPS value detected. The beta value estimate representing the response fixed effect suggests that NPS is lower for correct responses (no electric shock) than incorrect responses (electric shock), which is consistent with the purpose of the NPS to measure pain.

This may be the first time that a generalized neurologic pain signal has been connected to an electric shock stimulus.

## Comparison with Reward runs

However, this by itself does not demonstrate that the NPS reponse is in fact directly related to physical pain from the electric shock. Although there are a large number of possible alternative explanations, one plausible explanation is that the NPS signal change is simply that getting an item wrong in the game is affectively unpleasant, even without a direct physical pain. We might posit an ego-driven response to performing well in the task; in effect, an incorrect response is its own punishment.

In order to rule out this possibility, we need to compare the response we get during the punishment runs with a similar response in the reward runs. If we measure the NPS during the Reward task, and compare correct and incorrect responses, do we see the same or a different response? A significantly larger difference in NPS between incorrect and incorrect responses seems very unlikely and would suggest either a methdological 

or a difference in the opposite direction, i.e., 

